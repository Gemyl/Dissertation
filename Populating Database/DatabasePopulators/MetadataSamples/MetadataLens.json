{
    "total": 12524,
    "data": [
        {
            "lens_id": "000-899-066-182-539",
            "title": "Artificial intelligence and deep learning for small bowel capsule endoscopy.",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Standardization",
                "Capsule endoscopy",
                "Lesion detection",
                "Time saving",
                "Medicine",
                "Convolutional neural network"
            ],
            "keywords": [
                "artificial intelligence",
                "capsule endoscopy",
                "convolutional neural network",
                "deep learning",
                "machine learning"
            ],
            "abstract": "Capsule endoscopy is ideally suited to artificial intelligence based interpretation given its reliance on pattern recognition in still images. Time saving viewing modes and lesion detection features currently available rely on machine learning algorithms, a form of artificial intelligence. Current software, necessitates close human supervision given poor sensitivity relative to an expert reader. However, with the advent of deep learning, artificial intelligence is becoming increasingly reliable and will be increasingly relied upon. We review the major advances in artificial intelligence for capsule endoscopy in recent publications and briefly review artificial intelligence development for historical understanding. Importantly, recent advancements in artificial intelligence have not yet been incorporated into practice and it is immature to judge the potential of this technology based on current platforms. Remaining regulatory and standardization hurdles are being overcome and artificial intelligence based clinical applications are likely to proliferate rapidly.",
            "scholarly_citations_count": 14,
            "author_count": 2
        },
        {
            "lens_id": "002-800-794-748-190",
            "title": "Adversarial examples: attacks and defenses in the physical world",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Computational intelligence",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Deep neural networks",
                "Small magnitude",
                "Adversarial system",
                "Computer security",
                "Computer science",
                "Contextual image classification"
            ],
            "abstract": "Deep learning technology has become an important branch of artificial intelligence. However, researchers found that deep neural networks, as the core algorithm of deep learning technology, are vulnerable to adversarial examples. The adversarial examples are some special input examples which were added small magnitude and carefully crafted perturbations to yield erroneous results with extremely confidence. Hence, they bring serious security risks to deep-learning-based systems. Furthermore, adversarial examples exist not only in the digital world, but also in the physical world. This paper presents a comprehensive overview of adversarial attacks and defenses in the real physical world. First, we reviewed these works that can successfully generate adversarial examples in the digital world, analyzed the challenges faced by applications in real environments. Then, we compare and summarize the work of adversarial examples on image classification tasks, target detection tasks, and speech recognition tasks. In addition, the relevant feasible defense strategies are summarized. Finally, relying on the reviewed work, we propose potential research directions for the attack and defense of adversarial examples in the physical world.",
            "scholarly_citations_count": 34,
            "author_count": 3
        },
        {
            "lens_id": "003-122-125-862-945",
            "title": "On the complex domain deep machine learning for face recognition",
            "year_published": 2017,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Artificial intelligence",
                "Biometrics",
                "Facial recognition system",
                "Computer science",
                "Invariant (mathematics)",
                "Artificial neural network",
                "Feature extraction",
                "Classifier (UML)"
            ],
            "abstract": "Biometric based verification and recognition has become the center of attention for many significant applications for security conscious societies, as it is believed that biometrics can provide accurate and reliable identification. The face biometrics are one that possesses the merits of both high accuracy and low intrusiveness. An efficient machine recognition of human faces in big dataset is both important and challenging tasks. This paper addresses an intelligent face recognition system that is pose invariant and can recognize multi-expression, occluded and blurred faces through efficient but compact deep learning. Superior functionality of neural network in a complex domain has been observed in recent researches. My work presents a new approach, which is the fusion of higher-order novel neuron models with multivariate statistical techniques in a complex domain with a sole goal of improving performance of biometric systems. This also aims at reducing the computational cost and providing a faster recognition system. This paper presents the formal algorithms for feature extraction with multivariate statistical techniques in complex domain and compare them their real domain counterpart. This paper also presents a classifier structure (OCON : One-Class-in-One-Neuron) which contains an ensemble of novel higher order neurons, which drastically reduces the complexity of proposed learning machine because only single neuron is sufficient to recognize a subject in the database. This novel fusion in the proposed deep learning machine has thoroughly presented its superiority over a wide spectrum of experiments. Advanced deep learning capabilities, and complex domain implementation in particular, are significantly advancing state-of-art in computer vision and pattern recognition.",
            "scholarly_citations_count": 39,
            "author_count": 1
        },
        {
            "lens_id": "003-665-641-488-313",
            "title": "Guest Editors' Introduction: Special Section on Learning Deep Architectures",
            "year_published": 2013,
            "fields_of_study": [
                "Human\u2013computer interaction",
                "Deep learning",
                "Artificial intelligence",
                "Graphical model",
                "Instance-based learning",
                "Data science",
                "Algorithmic learning theory",
                "Computer science",
                "Computational learning theory",
                "Artificial neural network",
                "Feature learning",
                "Active learning (machine learning)",
                "Unsupervised learning"
            ],
            "abstract": "There has been a resurgence of research in the design of deep architecture models and learning algorithms, i.e., methods that rely on the extraction of a multilayer representation of the data. Often referred to as deep learning, this topic of research has been building on and contributing to many different research topics, such as neural networks, graphical models, feature learning, unsupervised learning, optimization, pattern recognition, and signal processing. Deep learning is also motivated and inspired by neuroscience and has had a tremendous impact on various applications such as computer vision, speech recognition, and natural language processing. The clearly multidisciplinary nature of deep learning led to a call for papers for a special issue dedicated to learning deep architectures.",
            "scholarly_citations_count": 30,
            "author_count": 5
        },
        {
            "lens_id": "008-406-868-044-480",
            "title": "ANNPR - Deep Learning in the Wild",
            "year_published": 2018,
            "fields_of_study": [
                "Software deployment",
                "Deep learning",
                "Machine perception",
                "Artificial intelligence",
                "Best practice",
                "Data science",
                "Control (management)",
                "Realm",
                "Quality (business)",
                "Computer science",
                "Artificial neural network"
            ],
            "abstract": "Deep learning with neural networks is applied by an increasing number of people outside of classic research environments, due to the vast success of the methodology on a wide range of machine perception tasks. While this interest is fueled by beautiful success stories, practical work in deep learning on novel tasks without existing baselines remains challenging. This paper explores the specific challenges arising in the realm of real world tasks, based on case studies from research & development in conjunction with industry, and extracts lessons learned from them. It thus fills a gap between the publication of latest algorithmic and methodical developments, and the usually omitted nitty-gritty of how to make them work. Specifically, we give insight into deep learning projects on face matching, print media monitoring, industrial quality control, music scanning, strategy game playing, and automated machine learning, thereby providing best practices for deep learning in practice.",
            "scholarly_citations_count": 26,
            "author_count": 11
        },
        {
            "lens_id": "009-723-153-412-572",
            "title": "Helmet Detection Based on Deep Learning and Random Forest on UAV for Power Construction Safety",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Artificial intelligence",
                "Power (physics)",
                "Random forest",
                "Computer science",
                "Convolutional neural network",
                "Construction site safety"
            ],
            "abstract": "<jats:p>Image recognition is one of the key technologies for worker\u2019s helmet detection using an unmanned aerial vehicle (UAV). By analyzing the image feature extraction method for workers\u2019 helmet detection based on convolutional neural network (CNN), a double-channel convolutional neural network (DCNN) model is proposed to improve the traditional image processing methods. On the basis of AlexNet model, the image features of the worker can be extracted using two independent CNNs, and the essential image features can be better reflected considering the abstraction degree of the features. Combining a traditional machine learning method and random forest (RF), an intelligent recognition algorithm based on DCNN and RF is proposed for workers\u2019 helmet detection. The experimental results show that deep learning (DL) is closely related to the traditional machine learning methods. Moreover, adding a DL module to the traditional machine learning framework can improve the recognition accuracy.</jats:p>",
            "scholarly_citations_count": 4,
            "author_count": 4
        },
        {
            "lens_id": "009-757-765-948-04X",
            "title": "Deep Learning Based High-Resolution Remote Sensing Image classification",
            "year_published": 2017,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Competitive learning",
                "Artificial intelligence",
                "Robot learning",
                "Instance-based learning",
                "Multi-task learning",
                "Computer science",
                "Computational learning theory",
                "Artificial neural network",
                "Feature learning",
                "Remote sensing",
                "Contextual image classification",
                "Inductive transfer",
                "Active learning (machine learning)",
                "Unsupervised learning"
            ],
            "abstract": "Abstract- Deep learning is an emerging research area in machine learning and pattern recognition field which has been presented with the goal of drawing Machine Learning nearer to one of its unique objectives, Artificial Intelligence. It tries to mimic the human brain, which is capable of processing and learning from the complex input data and solving different kinds of complicated tasks well. Deep learning (DL) basically based on a set of supervised and unsupervised algorithms that attempt to model higher level abstractions in data and make it self-learning for hierarchical representation for classification. In the recent years, it has attracted much attention due to its state-of-the-art performance in diverse areas like object perception, speech recognition, computer vision, collaborative filtering and natural language processing. This paper will present a survey on different deep learning techniques for remote sensing image classification.",
            "author_count": 1
        },
        {
            "lens_id": "010-092-322-021-247",
            "title": "An Empiric Analysis of Wavelet-Based Feature Extraction on Deep Learning and Machine Learning Algorithms for Arrhythmia Classification",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Artificial intelligence",
                "Computer science",
                "Wavelet",
                "Feature extraction"
            ],
            "scholarly_citations_count": 6,
            "author_count": 3
        },
        {
            "lens_id": "012-497-070-827-436",
            "title": "Deep Learning: An Application Perspective",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Set (psychology)",
                "Data analysis",
                "Data science",
                "Scope (project management)",
                "Field (computer science)",
                "Computer science",
                "Exabyte",
                "Artificial neural network"
            ],
            "abstract": "Nowadays, machine learning and deep learning have proven to be an encouraging field after artificial intelligence. Deep learning being operative, supervised, time and cost-effective technique evolving has its myriad applications not just in the field of pattern recognition and prediction but also can be utilized for addressing some important issues in the field of data science and data analytics. Deep learning came into existence with the availability of the exabyte of data collected with time. This subset technique of machine learning acquires the instructive, differential, and distinct features of data. Deep learning is a subset of machine learning that has made a noteworthy breakthrough as it is inspired by the working of the brain and its cells called neurons now coming up with significant performance, efficient evaluation and remarkable outcomes in a huge range of applications, and deep learning techniques have set their way with useful security tools and are now widely used in the field of statistics, neuroscience, data analytics, pattern recognition, image sensing, and the list goes on. This paper covers the basics of deep learning, its several commonly used algorithms and techniques, their architectures along with several application areas where these techniques have shown prominent differences and revolutionary change with the time. We have also tried to generalize the characteristics of different used methods of deep learning. At last, we present the challenges for the growth of these methods, concluding with our perspectives and future scope.",
            "scholarly_citations_count": 2,
            "author_count": 5
        },
        {
            "lens_id": "013-120-908-148-338",
            "title": "Hierarchical semi-Markov conditional random fields for deep recursive sequential data",
            "year_published": 2017,
            "fields_of_study": [
                "Regular conditional probability",
                "Algorithm",
                "Machine learning",
                "Random field",
                "Conditional random field",
                "Artificial intelligence",
                "Markov process",
                "Inference",
                "Time complexity",
                "Mathematics",
                "Variable-order Markov model",
                "Markov chain"
            ],
            "abstract": "We present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of linear-chain conditional random fields to model deep nested Markov processes. It is parameterised as a conditional log-linear model and has polynomial time algorithms for learning and inference. We derive algorithms for partially-supervised learning and constrained inference. We develop numerical scaling procedures that handle the overflow problem. We show that when depth is two, the HSCRF can be reduced to the semi-Markov conditional random fields. Finally, we demonstrate the HSCRF on two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. The HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases.",
            "scholarly_citations_count": 4,
            "author_count": 4
        },
        {
            "lens_id": "013-242-140-479-601",
            "title": "Cell Cycle-Regulated Genes Classification using Machine Learning and Deep Learning Techniques on Processed Microarrays Images",
            "year_published": 2021,
            "fields_of_study": [
                "Artificial intelligence",
                "Machine learning",
                "Computer science",
                "Deep learning",
                "Support vector machine",
                "Convolutional neural network",
                "Classifier (UML)",
                "DNA microarray",
                "Margin (machine learning)",
                "Artificial neural network",
                "Learning classifier system",
                "Random forest",
                "Field (mathematics)",
                "Online machine learning",
                "Gene",
                "Mathematics",
                "Biochemistry",
                "Gene expression",
                "Chemistry",
                "Pure mathematics"
            ],
            "abstract": "<jats:p>Nowadays, machine learning and deep learning algorithms, are considered as new technologies increasingly used in the biomedical field. Machine learning is a branch of Artificial Intelligence that aims to automatically find patterns in existing data. A new Machine Learning subfield, the deep learning theory, has emerged. It deals with object recognition in images. In this paper, our goal is DNA Microarrays\u2019analysis with these algorithms to classify two genes\u2019 types. The first class represents cell cycle regulated genes and the second is non cell cycle regulated ones. In the current state of the art, the researchers are processing the numerical data associated to gene evolution to achieve this classification. Here, we propose a new and different approach, based on the microarrays images\u2019 treatment. To classify images, we use three machine learning algorithms which are: Support Vector Machine, KNearest Neighbors and Random Forest Classifier. We also use the Convolutional Neural Network and the fully connected neural network algorithms. Experiments demonstrate that our approaches outperform the state of art by a margin of 14.73 per cent by using machine learning algorithms and a margin of 22.39 per cent by using deep learning models. Our models accomplish real time test accuracy of ~ 92.39 % at classifying using CNNand 94.73% using machine learning algorithms.</jats:p>"
        },
        {
            "lens_id": "013-299-102-627-623",
            "title": "Deep Kernel machines: a survey",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Kernel method",
                "Support vector machine",
                "Artificial intelligence",
                "Kernelization",
                "Computer science",
                "Artificial neural network",
                "Unsupervised learning",
                "Feature vector",
                "Kernel (statistics)"
            ],
            "abstract": "The emergence of deep learning frameworks paves the way for achieving higher-level data abstractions and possess the potential in consolidating both supervised and unsupervised learning paradigms. Researchers have made many successful explorations in the field of deep learning, with applications in the fields of face recognition, text mining, language translation, image prediction, and action recognition. Kernel machines act as a bridge between the linearity and nonlinearity for many machine learning algorithms such as support vector machines, extreme learning machines, and core vector machines. These Kernel machines play a vital role in mapping the data in the input space to a Kernel-induced high-dimensional feature space to obtain a better distribution of the data. In this Kernel-induced high-dimensional feature space, the distribution of data points will be more amenable to the classification problem under consideration. The Kernel trick facilitates in transforming the machine learning algorithms that require only inner product computations between the data vectors into a Kernel-based approach by selecting an appropriate Kernel function. In Kernel-based approaches, the Kernel functions can thus be utilized for accomplishing the inner product computations between the transformed data vectors in an implicitly defined Kernel-induced feature space. Unlike neural networks, the Kernel machines guarantee structural risk minimization and global optimal solutions. Also, the Kernel machines exhibit capabilities such as theoretical tractability and excellent performance in practical applications. These attempts motivated the researchers towards utilizing the emerging trends of deep learning with Kernel methods for building deep Kernel machines. Researchers integrate Kernel methods and deep learning networks for maintaining their advantages and make up their limitations, then apply the deep Kernel learning approaches for improving the performance of the learning algorithm in different applications. Different ways of building deep Kernel machines by integrating the Kernel methods and deep learning architectures include utilizing Kernel machines as the final classifier of deep learning networks, Kernelization in deep neural networks for better feature enrichment, and building deep Kernel machines by utilizing deep or multiple Kernels in different tasks. This survey attempts to provide an overview of different approaches in building several deep Kernel learning architectures for enhancing the learning algorithm properties and their performance in practical applications.",
            "scholarly_citations_count": 1,
            "author_count": 3
        },
        {
            "lens_id": "015-143-018-771-333",
            "title": "Artificial intelligence (AI) impacting diagnosis of glaucoma and understanding the regulatory aspects of AI-based software as medical device",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Visual field",
                "Software",
                "Optic neuropathy",
                "Glaucoma",
                "Optical coherence tomography",
                "Intraocular pressure",
                "Medical device",
                "Computer science"
            ],
            "keywords": [
                "Artificial intelligence",
                "Convolutional neural networks",
                "Deep learning",
                "Glaucoma diagnosis",
                "Machine learning",
                "Software as a medical device (SaMD)",
                "Support vector machine"
            ],
            "scholarly_citations_count": 15,
            "author_count": 3
        },
        {
            "lens_id": "015-473-859-621-155",
            "title": "Modulation pattern recognition of M-QAM signals based on convolutional neural network and extreme learning machine",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Short-time Fourier transform",
                "Pattern recognition",
                "Extreme learning machine",
                "Quadrature amplitude modulation",
                "Computer science",
                "Spectrogram",
                "Feature extraction",
                "QAM",
                "Convolutional neural network"
            ],
            "abstract": "In this paper, we propose a workflow and a deep learning algorithm for recognizing Quadrature amplitude modulation signal(QAM), this design adopts a convolutional neural network (CNN) and Extreme Learning Machine (ELM) as the core\uff0cleverage the powerful feature extraction of CNN and fast classification learning of ELM. The spectrogram image features of the signal obtained by short-time Fourier transform (STFT) are input to the CNN-ELM hybrid model, the modulation mode of the QAM signal is finally recognized by ELM. This algorithm surmounts the shortcomings of traditional methods well, Simulation results also verify the superiority of the proposed system whose classification accuracy is beyond 99.86%.",
            "author_count": 4
        },
        {
            "lens_id": "015-622-455-700-769",
            "title": "IWCIA - A Video Recognition Method by using Adaptive Structural Learning of Long Short Term Memory based Deep Belief Network",
            "year_published": 2019,
            "fields_of_study": [
                "Deep learning",
                "External Data Representation",
                "Artificial intelligence",
                "Test data",
                "Pattern recognition",
                "MNIST database",
                "Restricted Boltzmann machine",
                "Field (computer science)",
                "Computer science",
                "Artificial neural network",
                "Deep belief network"
            ],
            "abstract": "Deep learning builds deep architectures such as multi-layered artificial neural networks to effectively represent multiple features of input patterns. The adaptive structural learning method of Deep Belief Network (DBN) can realize a high classification capability while searching the optimal network structure during the training. The method can find the optimal number of hidden neurons of a Restricted Boltzmann Machine (RBM) by neuron generation-annihilation algorithm to train the given input data, and then it can make a new layer in DBN by the layer generation algorithm to actualize a deep data representation. Moreover, the learning algorithm of Adaptive RBM and Adaptive DBN was extended to the time-series analysis by using the idea of LSTM (Long Short Term Memory). In this paper, our proposed prediction method was applied to Moving MNIST, which is a benchmark data set for video recognition. We challenge to reveal the power of our proposed method in the video recognition research field, since video includes rich source of visual information. Compared with the LSTM model, our method showed higher prediction performance (more than 90% predication accuracy for test data).",
            "scholarly_citations_count": 2,
            "author_count": 2
        },
        {
            "lens_id": "015-899-508-094-689",
            "title": "AIM in Dermatology",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Applications of artificial intelligence",
                "Dermatology",
                "Teledermatology",
                "Skin cancer",
                "Digitization",
                "Telemedicine",
                "Computer science",
                "Convolutional neural network"
            ],
            "abstract": "In medicine, dermatology is a promising pioneer for the use of artificial intelligence (AI). In dermatological practice, the recognition of visual patterns (morphology) has always been fundamental for making a diagnosis, so artificial intelligence has great potential here. The collection of clinical data, especially image data, is playing an increasingly important role in the diagnosis and therapy of skin disease. Existing analog data archives are being digitized and restructured with great effort, and new data sets are often captured and labeled directly in digital form. During the last years, a growing number of studies have demonstrated AI\u2019s benefits in research settings, and first applications are already used clinically. Particularly in the detection of skin cancer and for the quantification of chronic inflammatory skin diseases, artificial intelligence is supporting doctors as well as patients to find the right diagnosis and treatment. The purpose of this book chapter is to discuss the potential applications of artificial intelligence in various areas of dermatology.\r\nKeywords\r\nDermatology Digitization Machine learning Artificial intelligence Deep learning Convolutional neural network Image recognition Pattern recognition Algorithm-based decision-making Skin cancer Psoriasis Eczema Teledermatology Telemedicine",
            "author_count": 3
        },
        {
            "lens_id": "016-647-542-440-122",
            "title": "Representation Learning: A Review and New Perspectives",
            "year_published": 2013,
            "fields_of_study": [
                "Autoencoder",
                "Deep learning",
                "Stability (learning theory)",
                "Machine learning",
                "Competitive learning",
                "Nonlinear dimensionality reduction",
                "Artificial intelligence",
                "Manifold",
                "Boltzmann machine",
                "Instance-based learning",
                "Inference",
                "Multi-task learning",
                "Algorithmic learning theory",
                "Sequence learning",
                "Computer science",
                "Probabilistic logic",
                "Computational learning theory",
                "Artificial neural network",
                "Feature extraction",
                "Semi-supervised learning",
                "Feature learning",
                "Inductive transfer",
                "Active learning (machine learning)",
                "Unsupervised learning"
            ],
            "abstract": "The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",
            "scholarly_citations_count": 8793,
            "author_count": 3
        },
        {
            "lens_id": "016-751-486-517-446",
            "title": "IWAIPR - Gait Speed Estimation Based on Artificial Neural Network: Comparison with the Application of the Inverted Pendulum Model",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Ranging",
                "Acceleration",
                "Artificial intelligence",
                "Pattern recognition",
                "Inverted pendulum",
                "Gait (human)",
                "Computer science",
                "Artificial neural network",
                "Angular velocity",
                "Inertial measurement unit"
            ],
            "abstract": "One of the most analyzed physical activities is the human gait. Among the different gait parameters, the gait speed is widely used in order to quantify improvements and specific functional impairments. Inertial Measurement Unit (IMU) sensors have been used for estimating the gait speed by means of different approaches ranging from physical gait models to machine learning/deep learning algorithms. In this paper, an algorithm based on artificial neural network (ANN) is proposed to be applied on the estimation of the gait speed. The features used by the network were extracted from acceleration and angular velocity signals supplied by a single IMU placed at the lumbar region. The results achieved by this application of a machine learning technique were compared with those obtained by the application of the inverted pendulum model. It proved that the proposed algorithm achieved better results. In order to train the ANN and to measure the performance achieved by both methods, a database, comprised by IMU signals recorded during the walking performed by young and elderly subjects, was built up. The ANN model proved the feasibility of its application on elderly subjects. Hence, it can be applied for monitoring frailty and screening cognitive diseases in older adults.",
            "author_count": 3
        },
        {
            "lens_id": "016-886-831-395-805",
            "title": "SASDL and RBATQ: Sparse Autoencoder With Swarm Based Deep Learning and Reinforcement Based Q-Learning for EEG Classification.",
            "year_published": 2022,
            "fields_of_study": [
                "Autoencoder",
                "Artificial intelligence",
                "Computer science",
                "Deep learning",
                "Electroencephalography",
                "Reinforcement learning",
                "Particle swarm optimization",
                "Machine learning",
                "Feature (linguistics)",
                "Pattern recognition (psychology)",
                "Deep belief network",
                "Schizophrenia (object-oriented programming)",
                "Psychology",
                "Neuroscience",
                "Linguistics",
                "Philosophy",
                "Programming language"
            ],
            "keywords": [
                "Deep learning",
                "EEG",
                "PSO",
                "Q-learning",
                "reinforcement learning"
            ],
            "abstract": "The most vital information about the electrical activities of the brain can be obtained with the help of Electroencephalography (EEG) signals. It is quite a powerful tool to analyze the neural activities of the brain and various neurological disorders like epilepsy, schizophrenia, sleep related disorders, parkinson disease etc. can be investigated well with the help of EEG signals. <i>Goal</i>: In this paper, two versatile deep learning methods are proposed for the efficient classification of epilepsy and schizophrenia from EEG datasets. <i>Methods</i>: The main advantage of using deep learning when compared to other machine learning algorithms is that it has the capability to accomplish feature engineering on its own. Swarm intelligence is also a highly useful technique to solve a wide range of real-world, complex, and non-linear problems. Therefore, taking advantage of these factors, the first method proposed is a Sparse Autoencoder (SAE) with swarm based deep learning method and it is named as (SASDL) using Particle Swarm Optimization (PSO) technique, Cuckoo Search Optimization (CSO) technique and Bat Algorithm (BA) technique; and the second technique proposed is the Reinforcement Learning based on Bidirectional Long-Short Term Memory (BiLSTM), Attention Mechanism, Tree LSTM and Q learning, and it is named as (RBATQ) technique. <i>Results and Conclusions</i>: Both these two novel deep learning techniques are tested on epilepsy and schizophrenia EEG datasets and the results are analyzed comprehensively, and a good classification accuracy of more than 93% is obtained for all the datasets.",
            "scholarly_citations_count": 2,
            "author_count": 2
        },
        {
            "lens_id": "017-712-764-965-666",
            "title": "Will machine learning end the viability of radiology as a thriving medical specialty",
            "year_published": 2018,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Engineering",
                "Artificial intelligence",
                "Specialty",
                "Thriving"
            ],
            "abstract": "There have been tremendous advances in artificial intelligence (AI) and machine learning (ML) within the past decade, especially in the application of deep learning to various challenges. These inc...",
            "scholarly_citations_count": 50,
            "author_count": 2
        },
        {
            "lens_id": "019-060-941-940-286",
            "title": "Handwritten Digit Recognition using Convolutional Neural Network in Python with Tensorflow and Observe the Variation of Accuracies for Various Hidden Layers",
            "year_published": 2019,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Backpropagation",
                "Pattern recognition",
                "MNIST database",
                "Stochastic gradient descent",
                "Digit recognition",
                "Computer science",
                "Python (programming language)",
                "Convolutional neural network"
            ],
            "abstract": "<jats:p>In recent times, with the increase of Artificial Neural Network (ANN), deep learning has brought a dramatic twist in the field of machine learning by making it more Artificial Intelligence (AI). Deep learning is used remarkably used in vast ranges of fields because of its diverse range of applications such as surveillance, health, medicine, sports, robotics, drones etc. In deep learning, Convolutional Neural Network (CNN) is at the center of spectacular advances that mixes Artificial Neural Network (ANN) and up to date deep learning strategies. It has been used broadly in pattern recognition, sentence classification, speech recognition, face recognition, text categorization, document analysis, scene, and handwritten digit recognition. The goal of this paper is to observe the variation of accuracies of CNN to classify handwritten digits using various numbers of hidden layer and epochs and to make the comparison between the accuracies. For this performance evaluation of CNN, we performed our experiment using Modified National Institute of Standards and Technology (MNIST) dataset. Further, the network is trained using stochastic gradient descent and the backpropagation algorithm.</jats:p>",
            "scholarly_citations_count": 5,
            "author_count": 3
        },
        {
            "lens_id": "019-483-198-943-871",
            "title": "Machines Who Learn.",
            "year_published": 2016,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Computer science",
                "Artificial neural network"
            ],
            "abstract": "The article discusses artificial intelligence (AI) and the machine learning known as deep learning, referencing the history of AI from the 1950s through the mid 2010s and the algorithms used in deep learning. The use of artificial neural networks in deep learning, including in regard to training a neural network to recognize faces and patterns in static images, is discussed.",
            "scholarly_citations_count": 10,
            "author_count": 1
        },
        {
            "lens_id": "019-785-375-589-070",
            "title": "Margined winner-take-all: New learning rule for pattern recognition.",
            "year_published": 2017,
            "fields_of_study": [
                "Neocognitron",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Winner-take-all",
                "Layer (object-oriented design)",
                "Pattern recognition",
                "Learning rule",
                "Computer science",
                "Margin (machine learning)",
                "Convolutional neural network"
            ],
            "keywords": [
                "Deep CNN",
                "Interpolating-vector",
                "Learning rule",
                "Margined WTA",
                "Neocognitron",
                "Pattern recognition"
            ],
            "scholarly_citations_count": 5,
            "author_count": 1
        },
        {
            "lens_id": "022-140-920-472-565",
            "title": "Deep Learning-based Multi-focus Image Fusion: A Survey and A Comparative Study",
            "year_published": 2022,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Artificial intelligence",
                "Multi focus",
                "Field (computer science)",
                "Computer science",
                "Image fusion",
                "Image processing"
            ],
            "abstract": "Multi-focus image fusion (MFIF) is an important area in image processing. Since 2017, deep learning has been introduced to the field of MFIF and various methods have been proposed. However, there is a lack of survey papers that discuss deep learning-based MFIF methods in detail. In this study, we fill this gap by giving a detailed survey on deep learning-based MFIF algorithms, including categories, methods, datasets and evaluation metrics. To the best of our knowledge, this is the first survey paper which focuses on deep learning based approaches in the field of MFIF. Besides, extensive experiments have been conducted to compare the performances of deep learning-based MFIF algorithms with conventional MFIF approaches. By analyzing qualitative and quantitative results, we give some observations on the current status of MFIF and discuss some future prospects of this field.",
            "scholarly_citations_count": 23,
            "author_count": 1
        },
        {
            "lens_id": "025-884-057-798-02X",
            "title": "Research and Application of Ancient Chinese Pattern Restoration Based on Deep Convolutional Neural Network.",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Convolutional neural network",
                "Artificial intelligence",
                "Computer science",
                "Artificial neural network",
                "Preprocessor",
                "Pattern recognition (psychology)",
                "Machine learning",
                "Field (mathematics)",
                "Data pre-processing",
                "Sample (material)",
                "Mathematics",
                "Chemistry",
                "Chromatography",
                "Pure mathematics"
            ],
            "abstract": "In recent years, deep learning, as a very popular artificial intelligence method, can be said to be a small area in the field of image recognition. It is a type of machine learning, actually derived from artificial neural networks, and is a method used to learn the characteristics of sample data. It is a multilayer network, which can learn the information from the bottom to the top of the image through the multilayer network, so as to extract the characteristics of the sample, and then perform identification and classification. The purpose of deep learning is to make the machine have the same analytical and learning capabilities as the human brain. The ability of deep learning in data processing (including images) is unmatched by other methods, and its achievements in recent years have left other methods behind. This article comprehensively reviews the application research progress of deep convolutional neural networks in ancient Chinese pattern restoration and mainly focuses on the research based on deep convolutional neural networks. The main tasks are as follows: (1) a detailed and comprehensive introduction to the basic knowledge of deep convolutional neural and a summary of related algorithms along the three directions of text preprocessing, learning, and neural networks are provided. This article focuses on the related mechanism of traditional pattern repair based on deep convolutional neural network and analyzes the key structure and principle. (2) Research on image restoration models based on deep convolutional networks and adversarial neural networks is carried out. The model is mainly composed of four parts, namely, information masking, feature extraction, generating network, and discriminant network. The main functions of each part are independent and interdependent. (3) The method based on the deep convolutional neural network and the other two methods are tested on the same part of the Qinghai traditional embroidery image data set. From the final evaluation index of the experiment, the method in this paper has better evaluation index than the traditional image restoration method based on samples and the image restoration method based on deep learning. In addition, from the actual image restoration effect, the method in this paper has a better image restoration effect than the other two methods, and the restoration results produced are more in line with the habit of human observation with the naked eye.",
            "scholarly_citations_count": 2,
            "author_count": 1
        },
        {
            "lens_id": "026-587-473-383-873",
            "title": "A prototype to detect anomalies using machine learning algorithms and deep neural network",
            "year_published": 2018,
            "fields_of_study": [
                "Autoencoder",
                "Deep learning",
                "Algorithm",
                "Machine learning",
                "Supervised learning",
                "Artificial intelligence",
                "One-class classification",
                "Computer science",
                "Artificial neural network",
                "Dimensionality reduction",
                "Big data",
                "Unsupervised learning"
            ],
            "abstract": "Artificial Intelligence is making a huge impact nowadays in almost all the applications. It is all about instructing a machine to perceive an object like a human. Making the machine to excel at this perception requires training it by feeding large number of examples. In this way machine learning algorithms find many applications for real time problems. In the last five years, Deep Learning techniques have transfigured the field of machine learning, data mining and big data analytics. This research is to investigate the presence of anomalies in given data. This model can be used as a prototype and can be applied in domains like finding abnormalities in medical tests, to segregate fraud applications in banking, insurance records, to monitor IT infrastructure, observing energy consumption and vehicle tracking. The concepts of supervised learning and unsupervised learning are used with the help of machine learning algorithms and deep neural networks. Even though machine learning algorithms are effective for classification problems, the data in question, determines the efficiency of these algorithms. It has been showed in this paper; the traditional machine learning algorithms fail when the data is highly imbalanced and necessitate the use of deep neural networks. One such neural network called deep Autoencoder, is used to detect anomalies present in a large data set which is largely biased. The results derived out of this study, proved to be very accurate.",
            "scholarly_citations_count": 4,
            "author_count": 3
        },
        {
            "lens_id": "030-371-315-182-322",
            "title": "Special Issue on Artificial Intelligence",
            "year_published": 2019,
            "fields_of_study": [
                "Artificial intelligence",
                "Computer science"
            ],
            "abstract": "<jats:p>Artificial intelligence (AI) is an interdisciplinary subject in science and engineering that makes it possible for machines to learn from data. Artificial Intelligence applications include prediction, recommendation, classification and recognition, object detection, natural language processing, autonomous systems, among others. The topics of the articles in this special issue include deep learning applied to medicine [1, 3], support vector machine applied to ecosystems [2], human-robot interaction [4], clustering in the identification of anomalous patterns in communication networks [5], expert systems for the simulation of natural disaster scenarios [6], real-time algorithms of artificial intelligence [7] and big data analytics for natural disasters [8].</jats:p>",
            "author_count": 1
        },
        {
            "lens_id": "030-597-915-675-886",
            "title": "On the challenge of learning complex functions.",
            "year_published": 2007,
            "fields_of_study": [
                "Curse of dimensionality",
                "Machine learning",
                "Computational neuroscience",
                "Artificial intelligence",
                "Instance-based learning",
                "Algorithmic learning theory",
                "Computer science",
                "Boosting (machine learning)",
                "Computational learning theory",
                "Artificial neural network",
                "Deep belief network"
            ],
            "abstract": "A common goal of computational neuroscience and of artificial intelligence research based on statistical learning algorithms is the discovery and understanding of computational principles that could explain what we consider adaptive intelligence, in animals as well as in machines. This chapter focuses on what is required for the learning of complex behaviors. We believe it involves the learning of highly varying functions, in a mathematical sense. We bring forward two types of arguments which convey the message that many currently popular machine learning approaches to learning flexible functions have fundamental limitations that render them inappropriate for learning highly varying functions. The first issue concerns the representation of such functions with what we call shallow model architectures. We discuss limitations of shallow architectures, such as so-called kernel machines, boosting algorithms, and one-hidden-layer artificial neural networks. The second issue is more focused and concerns kernel machines with a local kernel (the type used most often in practice) that act like a collection of template-matching units. We present mathematical results on such computational architectures showing that they have a limitation similar to those already proved for older non-parametric methods, and connected to the so-called curse of dimensionality. Though it has long been believed that efficient learning in deep architectures is difficult, recently proposed computational principles for learning in deep architectures may offer a breakthrough.",
            "scholarly_citations_count": 15,
            "author_count": 1
        },
        {
            "lens_id": "033-669-184-407-683",
            "title": "Deep kernel learning in extreme learning machines",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Kernel method",
                "Support vector machine",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Extreme learning machine",
                "Computer science",
                "Computation",
                "Polynomial",
                "Kernel (statistics)"
            ],
            "abstract": "Emergence of extreme learning machine as a breakneck learning algorithm has marked its prominence in solitary hidden layer feed-forward networks. Kernel-based extreme learning machine (KELM) reflected its efficiency in diverse applications where feature mapping functions of hidden nodes are concealed from users. The conventional KELM algorithms involve only solitary layer of kernels, thereby emulating shallow learning architectures for its feature transformation. Trend in migrating shallow-based learning models into deep learning architectures opens up a new outlook for machine learning domains. This paper attempts to bestow deep kernel learning approach in a conventional shallow architecture. The emerging arc-cosine kernels possess the potential to mimic the prevailing deep layered frameworks to a greater extent. Unlike other kernels such as linear, polynomial and Gaussian, arc-cosine kernels have a recursive nature by itself and have the potential to express multilayer computation in learning models. This paper explores the possibility of building a new deep kernel machine with extreme learning machine and multilayer arc-cosine kernels. This framework outperforms conventional KELM and deep support vector machine in terms of training time and accuracy.",
            "scholarly_citations_count": 17,
            "author_count": 3
        },
        {
            "lens_id": "035-112-585-769-997",
            "title": "Training fuzzy deep neural network with honey badger algorithm for intrusion detection in cloud environment",
            "year_published": 2023,
            "fields_of_study": [
                "Computer science",
                "Cloud computing",
                "Intrusion detection system",
                "Algorithm",
                "Data mining",
                "Artificial neural network",
                "Machine learning",
                "Fuzzy logic",
                "Computational intelligence",
                "Computer security",
                "Artificial intelligence",
                "Operating system"
            ],
            "author_count": 3
        },
        {
            "lens_id": "035-214-296-603-090",
            "title": "Wild patterns: Ten years after the rise of adversarial machine learning",
            "year_published": 2018,
            "fields_of_study": [
                "Adversarial system",
                "Adversarial machine learning",
                "Artificial intelligence",
                "Machine learning",
                "Computer science",
                "Deep learning",
                "Context (archaeology)",
                "Vulnerability (computing)",
                "Field (mathematics)",
                "Work (physics)",
                "Computer security",
                "Engineering",
                "Mechanical engineering",
                "Paleontology",
                "Mathematics",
                "Pure mathematics",
                "Biology"
            ],
            "abstract": "Learning-based pattern classifiers, including deep networks, have shown impressive performance in several application domains, ranging from computer vision to cybersecurity. However, it has also been shown that adversarial input perturbations carefully crafted either at training or at test time can easily subvert their predictions. The vulnerability of machine learning to such wild patterns (also referred to as adversarial examples), along with the design of suitable countermeasures, have been investigated in the research field of adversarial machine learning. In this work, we provide a thorough overview of the evolution of this research area over the last ten years and beyond, starting from pioneering, earlier work on the security of non-deep learning algorithms up to more recent work aimed to understand the security properties of deep learning algorithms, in the context of computer vision and cybersecurity tasks. We report interesting connections between these apparently-different lines of work, highlighting common misconceptions related to the security evaluation of machine-learning algorithms. We review the main threat models and attacks defined to this end, and discuss the main limitations of current work, along with the corresponding future challenges towards the design of more secure learning algorithms.",
            "scholarly_citations_count": 484,
            "author_count": 2
        },
        {
            "lens_id": "037-947-053-049-903",
            "title": "Artificial Intelligence Based Glaucoma Detection",
            "year_published": 2019,
            "fields_of_study": [
                "Deep learning",
                "Transfer of learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Glaucoma",
                "Learning potential",
                "Computer science",
                "Ensemble learning",
                "Training set",
                "Medical diagnosis"
            ],
            "abstract": "Artificial Intelligence has shown significant enhancement recently, in many fields, especially in pattern recognition in computer vision and hence in medical diagnosis, wherein highly accurate automated diseases diagnosis has now become feasible. Artificial Intelligence methods, especially based on deep learning are very powerful and have got huge training and learning potential. These methods are growing from strength to strength and far exceeding the performance of traditional methods. The deep learning has proved to be highly efficient and accurate especially when balanced and large training data is available. This chapter provides an overview of various Artificial Intelligence based techniques for detection of the second-largest disease which causes blindness, that is, glaucoma. The chapter begins with the introduction of glaucoma and the various important parameters required for its accurate diagnosis. The later sections introduce various traditional Machine learning based techniques, followed by Deep learning based techniques including the Transfer learning and Ensemble learning based techniques in use for automated glaucoma detection, along with their advantages and drawbacks. This chapter also exposes readers to original and genuine data of perimetry and OCT tests.",
            "scholarly_citations_count": 5,
            "author_count": 2
        },
        {
            "lens_id": "039-252-737-134-49X",
            "title": "Review of Optical Character Recognition for Power System Image Based on Artificial Intelligence Algorithm",
            "year_published": 2023,
            "fields_of_study": [
                "Optical character recognition",
                "Computer science",
                "Artificial intelligence",
                "Character recognition",
                "Artificial neural network",
                "Character (mathematics)",
                "Intelligent character recognition",
                "Generalization",
                "Pattern recognition (psychology)",
                "Image (mathematics)",
                "Image processing",
                "Algorithm",
                "Machine learning",
                "Intelligent word recognition",
                "Mathematics",
                "Mathematical analysis",
                "Geometry"
            ],
            "abstract": "Optical Character Recognition (OCR) refers to a technology that uses image processing technology and character recognition algorithms to identify characters on an image. This paper is a deep study on the recognition effect of OCR based on Artificial Intelligence (AI) algorithms, in which the different AI algorithms for OCR analysis are classified and reviewed. Firstly, the mechanisms and characteristics of artificial neural network-based OCR are summarized. Secondly, this paper explores machine learning-based OCR, and draws the conclusion that the algorithms available for this form of OCR are still in their infancy, with low generalization and fixed recognition errors, albeit with better recognition effect and higher recognition accuracy. Finally, this paper explores several of the latest algorithms such as deep learning and pattern recognition algorithms. This paper concludes that OCR requires algorithms with higher recognition accuracy.",
            "author_count": 3
        },
        {
            "lens_id": "039-329-936-584-832",
            "title": "An Evaluation of Deep Learning Miniature Concerning in Soft Computing",
            "year_published": 2015,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Competitive learning",
                "Support vector machine",
                "Soft computing",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Computer science",
                "Artificial neural network",
                "Convolutional neural network",
                "Deep belief network"
            ],
            "abstract": "In recent years, Deep Learning at the latest developed field belonging to soft computing. The Deep learning has been a hot topic in the communities of artificial intelligence, artificial neural networks and machine learning. It tries to mimic the human brain, which is capable of processing the intricate input data, learning various knowledge's intellectually and intense as well as solving sundry kinds of sophisticated tasks well. The deep learning paradigm tackles problems in which shallow architectures (e.g. SVM) are impressed with the curse of dimensionality. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. The transformation these characteristics of the human brain to a learning model, we wish the model can deal with the high-dimensional data, support an intense and intellectual learning algorithm and perform well in the inextricable artificial intelligence, real tasks, such as pattern recognition speech recognition, image classification, computer vision and natural language processing. In this paper, we are discussing the history of deep learning, Deep Learning Architectures, abridge the components of Deep Boltzmann Machines (DBM), Deep Stacking Networks (DSN), Compound Hierarchical Deep Models(CHDM), Deep Convolutional Neural Network (DCNN) and Deep Belief Network (DBN) their learning algorithms.",
            "scholarly_citations_count": 8,
            "author_count": 1
        },
        {
            "lens_id": "039-870-391-197-431",
            "title": "Learning with Hierarchical-Deep Models",
            "year_published": 2013,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Hierarchical Dirichlet process",
                "Artificial intelligence",
                "Prior probability",
                "Boltzmann machine",
                "Inference",
                "One-shot learning",
                "Computer science",
                "Feature learning",
                "Cognitive neuroscience of visual object recognition"
            ],
            "abstract": "We introduce HD (or \u201cHierarchical-Deep\u201d) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian (HB) models. Specifically, we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training example by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.",
            "scholarly_citations_count": 205,
            "author_count": 3
        },
        {
            "lens_id": "040-111-160-394-671",
            "title": "Learning to Relate Images",
            "year_published": 2013,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Artificial intelligence",
                "Pattern recognition",
                "Bilinear interpolation",
                "Computer science",
                "Invariant (mathematics)",
                "Visual odometry",
                "Feature learning",
                "Stereopsis"
            ],
            "abstract": "A fundamental operation in many vision tasks, including motion understanding, stereopsis, visual odometry, or invariant recognition, is establishing correspondences between images or between images and data from other modalities. Recently, there has been increasing interest in learning to infer correspondences from data using relational, spatiotemporal, and bilinear variants of deep learning methods. These methods use multiplicative interactions between pixels or between features to represent correlation patterns across multiple images. In this paper, we review the recent work on relational feature learning, and we provide an analysis of the role that multiplicative interactions play in learning to encode relations. We also discuss how square-pooling and complex cell models can be viewed as a way to represent multiplicative interactions and thereby as a way to encode relations.",
            "scholarly_citations_count": 95,
            "author_count": 1
        },
        {
            "lens_id": "042-367-371-022-108",
            "title": "Application of Machine Learning for Fault Detection and Energy Efficiency Improvement in HVAC Application",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "HVAC",
                "Fault detection and isolation",
                "Artificial intelligence",
                "Telecommunications equipment",
                "Efficient energy use",
                "Sustainable living",
                "Bayesian network",
                "Computer science",
                "Acronym"
            ],
            "abstract": "The technology augmenting the human civilization in a significant stage is none other than artificial intelligence which has adopted human thinking level power and working as per level of the human brain, sometimes more than that also. From this point of view, the birth of machine learning and deep learning happens which are part of AI and which are basically the tool where machine learns from a human being based on data, pattern, and images. Sometimes it is very difficult for the human being to handle a huge amount of data manually, then the importance of machines is realized. The fields of machine learning, deep learning, and artificial intelligence have seen significant breakthroughs in the past decade. New computer science techniques, such as deep learning and Bayesian networks, have revolutionized processes in many industries. Medical diagnostics, speech recognition, and financial forecasting have all enjoyed major advances from the use of these algorithms. HVAC industry is one of the major industries where machine learning and deep learning applications are very much essential. The acronym HVAC stands for heating, ventilation, and air-conditioning. HVAC systems are responsible for the continuous regulation of the environment surrounding your telecom equipment. As the outside weather changes, the environment in which your equipment is housed will change as well. The chapter presents the effects of AI in daily human life for HVAC application for sustainable living. It also presents how the faults in the system can be identified by the machine learning algorithm.",
            "scholarly_citations_count": 2,
            "author_count": 4
        },
        {
            "lens_id": "044-867-461-903-244",
            "title": "Research on Text Detection and Recognition Based on OCR Recognition Technology",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Object detection",
                "Feature (machine learning)",
                "Computer science",
                "Machine vision",
                "Optical character recognition",
                "Contextual image classification",
                "Segmentation",
                "Image processing"
            ],
            "abstract": "Optical character recognition (OCR) is an important branch in the field of machine vision. It involves pattern recognition, image processing, digital signal processing, artificial intelligence and other disciplines. It is a comprehensive. It has important use value and theoretical significance in high-tech fields such as word information processing, office automation, machine translation and real-time monitoring system. In the 21st century, with the popularity of smart phones with high-definition cameras, OCR has a new pursuit in its development: more and more people pick up their mobile phones to photograph the things and scenes they see and obtain the text information in the pictures. Therefore, the recognition of characters in natural scenes has become a brand-new topic. In the past, text detection and text recognition algorithms were basically based on artificially designed features and traditional image processing methods. These features and algorithms were difficult to design and needed a lot of professional knowledge and experience support, so the accuracy was not high and they were not generalized. In recent years, with the rapid development of deep learning technology, breakthroughs have been made in the fields of computer vision such as image classification, object detection and semantic segmentation. Deep learning algorithm is a data-driven algorithm. The algorithm based on deep learning can automatically discover and learn the hidden feature rules in a large number of data through iterative training, without too much human intervention, so it has better generalization than traditional image processing related algorithms.",
            "scholarly_citations_count": 4,
            "author_count": 1
        },
        {
            "lens_id": "047-358-725-753-854",
            "title": "Artificial intelligence: the future for cardiology.",
            "year_published": 2019,
            "fields_of_study": [
                "Human intelligence",
                "Internal medicine",
                "Variety (cybernetics)",
                "Recurrent neural network",
                "Artificial intelligence",
                "Cardiology",
                "Workflow",
                "Coronary angiography",
                "Disease risk",
                "Field (computer science)",
                "Artificial neural network",
                "Medicine"
            ],
            "keywords": [
                "artificial intelligence",
                "cardiology",
                "coronary angiography",
                "echocardiography"
            ],
            "abstract": "Artificial intelligence (AI) is a rapidly evolving field in medicine, especially cardiology. AI is defined as the \u2018theory and development of computer systems able to perform tasks normally requiring human intelligence\u2019.1 By creating algorithms using a variety of neural networks, we enable dynamic deep machine learning.2 3 Deep machine learning mimics the human brain by using large high-quality data sets to create layers of neural networks to generate automated predictions, interpret image data and develop pattern recognition.3\u20136 AI aims to aid cardiologists in making better decisions, improve workflow, productivity, and cost-effectiveness and ultimately patient outcomes.2 4 7 \n\nAI has been used in automated predictions of cardiovascular disease risk scores and heart failure diagnosis. Using recurrent neural networks, deep machine learning has \u2026",
            "scholarly_citations_count": 11,
            "author_count": 1
        },
        {
            "lens_id": "052-119-120-824-149",
            "title": "Implementation of Deep Learning Algorithm with Perceptron using TenzorFlow Library",
            "year_published": 2019,
            "fields_of_study": [
                "Deep learning",
                "Algorithm",
                "Audio signal processing",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Connectionist expert system",
                "Field (computer science)",
                "Computer science",
                "Artificial neural network",
                "Feature extraction",
                "Perceptron"
            ],
            "abstract": "In recent years, Deep Learning, Machine Learning, and Artificial Intelligence are highly focused concepts of data science. Deep learning has achieved success in the field of Computer Vision, Speech and Audio Processing, and Natural Language Processing. It has the strong learning ability that can improve utilization of datasets for the feature extraction compared to traditional Machine Learning Algorithm. Perceptron is the essential building block for creating a deep Neural Network. The perceptron model is the more general computational model. It analyzes the unsupervised data, making it a valuable tool for data analytics. A key task of this paper is to develop and analyze learning algorithm. It begins with deep learning with perceptron and how to apply it using TensorFlow to solve various issues. The main part of this paper is to make perceptron learning algorithm well behaved with non-separable training datasets. This type of algorithm is suitable for Machine Learning, Deep Learning, Pattern Recognition, and Connectionist Expert System.",
            "scholarly_citations_count": 13,
            "author_count": 3
        },
        {
            "lens_id": "054-982-038-561-124",
            "title": "Designing neural networks through neuroevolution",
            "year_published": 2019,
            "fields_of_study": [
                "Deep learning",
                "Neuroevolution",
                "Artificial intelligence",
                "Backpropagation",
                "Evolutionary algorithm",
                "Stochastic gradient descent",
                "Population",
                "Computer science",
                "Artificial neural network",
                "Reinforcement learning"
            ],
            "abstract": "Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field\u2019s contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence. Deep neural networks have become very successful at certain machine learning tasks partly due to the widely adopted method of training called backpropagation. An alternative way to optimize neural networks is by using evolutionary algorithms, which, fuelled by the increase in computing power, offers a new range of capabilities and modes of learning.",
            "scholarly_citations_count": 399,
            "author_count": 4
        },
        {
            "lens_id": "055-388-200-101-009",
            "title": "Cryptographic Algorithms Identification based on Deep Learning",
            "year_published": 2022,
            "fields_of_study": [
                "Computer science",
                "Cryptography",
                "Neural cryptography",
                "Identification (biology)",
                "Block cipher",
                "Cryptanalysis",
                "Algorithm",
                "Differential cryptanalysis",
                "Artificial neural network",
                "Artificial intelligence",
                "Public-key cryptography",
                "Encryption",
                "Computer security",
                "Botany",
                "Biology"
            ],
            "abstract": "<jats:p>The identification of cryptographic algorithms is the premise of cryptanalysis which can help recover the keys effectively. This paper focuses on the construction of cryptographic identification classifiers based on residual neural network and feature engineering. We select 6 algorithms including block ciphers and public keys ciphers for experiments. The results show that the accuracy is generally over 90% for each algorithm. Our work has successfully combined deep learning with cryptanalysis, which is also very meaningful for the development of modern cryptography and pattern recognition.</jats:p>",
            "author_count": 3
        },
        {
            "lens_id": "055-976-086-300-100",
            "title": "Artificial neural networks in the modern world: features of application",
            "year_published": 2021,
            "fields_of_study": [
                "Artificial intelligence",
                "Artificial neural network",
                "Computer science",
                "Deep learning",
                "Machine learning",
                "Field (mathematics)",
                "Workflow",
                "Mathematics",
                "Database",
                "Pure mathematics"
            ],
            "abstract": "<jats:p>This article discusses the peculiarities of the use of artificial neural networks in the modern world. Artificial neural networks are widely used in areas where traditional computers do not work very well. For example, to solve problems where instead of programmed results it is necessary that the system studies, adapts and changes the results depending on the data it receives. Neural networks are also widely used when talking about working with noisy or incomplete data. Due to the ability to learn and adapt, similar to the brain, neural networks form a complete basis and are used in artificial intelligence and, consequently, in machine learning algorithms. As we collect more and more data every year, it makes sense to use deep learning models. In addition, the field of deep learning is also developing rapidly. Many researchers are working hard to develop excellent models of deep learning for specific problems. The use of deep neural networks and other machine learning technologies to solve long-standing problems is evolving rapidly and promises to shape the future of technology. Modern technologies use both controlled and uncontrolled methods. They are likely to become central to practice, and will soon become as widespread and inconspicuous as other technologies we have integrated into everyday use. The scale of the problems that machine learning can help solve is enormous and is likely to evolve rapidly. Integrating machine learning into the daily workflow can expand our capabilities and make modern problem-solving methods more efficient, more focused on higher-order tasks. Neural networks solve problems that require pattern recognition. They are good for pattern recognition, classification and optimization. These include handwriting recognition, face recognition, speech recognition, text translation, credit card fraud detection, medical diagnostics and huge data solutions. You can use it to find connections between templates, convert one type of data to another, and create associations or generalizations between different entities. The article will consider the application of artificial neural networks in various industries.</jats:p>",
            "author_count": 1
        },
        {
            "lens_id": "060-189-482-133-98X",
            "title": "A Systematic Review of Deep Learning for Silicon Wafer Defect Recognition",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Supervised learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Network architecture",
                "Computer science",
                "Feature extraction",
                "Cluster analysis",
                "Unsupervised learning",
                "Convolutional neural network"
            ],
            "abstract": "Advancements in technology have made deep learning a hot research area, and we see its applications in various fields. Its widespread use in silicon wafer defect recognition is replacing traditional machine learning and image processing methods of defect monitoring. This article presents a review of the deep learning methods employed for wafer map defect recognition. A systematic literature review (SLR) has been conducted to determine how the semiconductor industry is leveraged by deep learning research advancements for wafer defects recognition and analysis. Forty-four articles from well-known databases have been selected for this review. The articles\u2019 detailed study identified the prominent deep learning algorithms and network architectures for wafer map defect classification, clustering, feature extraction, and data synthesis. The identified learning algorithms are grouped as supervised learning, unsupervised learning, and hybrid learning. The network architectures include different forms of Convolutional Neural Network (CNN), Generative Adversarial Network (GAN), and Auto-encoder (AE). Various issues of multi-class and multi-label defects have been addressed, solving data unavailability, class imbalance, instance labeling, and unknown defects. For future directions, it is recommended to invest more efforts in the accuracy of the data generation procedures and the defect pattern recognition frameworks for defect monitoring in real industrial environments.",
            "scholarly_citations_count": 10,
            "author_count": 6
        },
        {
            "lens_id": "060-697-047-284-389",
            "title": "Tolerance-based granular methods: Foundations and applications in natural language processing",
            "year_published": 2023,
            "abstract": "<jats:p>Natural Language processing (NLP) derives its roots from artificial intelligence and computational linguistics. The proliferation of large-scale web corpora and social media data as well as advances in machine learning and deep learning have led to practical applications in diverse NLP areas such as machine translation, information extraction, named entity recognition (NER), text summarization and sentiment analysis. Named-entity recognition (NER), is a sub task of information extraction that seeks to discover and categorize specific entities such as nouns or relations in unstructured text. In this paper, we present a review of the foundations three tolerance-based granular computing methods (rough sets, fuzzy-rough sets and near sets) for representing structured (documents) and unstructured (linguistic entities) text. Applications of these methods are presented via semi-supervised and supervised learning algorithms in labelling relational facts from web corpora and sentiment classification (non-topic based text). The performance of the three presented algorithms is discussed in terms of bench marked datasets and algorithms. We make the case that tolerance relations provide an ideal framework for studying the concept of similarity for text-based applications. The aim of our work is to demonstrate that approximation structures viewed through the prism of tolerance have a great deal of fluidity and integrate conceptual structures at different levels of granularity thereby facilitating learning in the presented NLP applications.</jats:p>",
            "author_count": 1
        },
        {
            "lens_id": "065-069-967-200-162",
            "title": "Drug discovery with explainable artificial intelligence",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Computer science",
                "Bespoke",
                "Interpretation (philosophy)",
                "Cheminformatics",
                "Drug discovery",
                "Function (engineering)"
            ],
            "abstract": "Deep learning bears promise for drug discovery, including advanced image analysis, prediction of molecular structure and function, and automated generation of innovative chemical entities with bespoke properties. Despite the growing number of successful prospective applications, the underlying mathematical models often remain elusive to interpretation by the human mind. There is a demand for \u2018explainable\u2019 deep learning methods to address the need for a new narrative of the machine language of the molecular sciences. This Review summarizes the most prominent algorithmic concepts of explainable artificial intelligence, and forecasts future opportunities, potential applications as well as several remaining challenges. We also hope it encourages additional efforts towards the development and acceptance of explainable artificial intelligence techniques. Drug discovery has recently profited greatly from the use of deep learning models. However, these models can be notoriously hard to interpret. In this Review, Jimenez-Luna and colleagues summarize recent approaches to use explainable artificial intelligence techniques in drug discovery.",
            "scholarly_citations_count": 233,
            "author_count": 3
        },
        {
            "lens_id": "065-194-677-112-263",
            "title": "Small target deep convolution recognition algorithm based on improved YOLOv4",
            "year_published": 2022,
            "fields_of_study": [
                "Computer science",
                "Pooling",
                "Artificial intelligence",
                "Pattern recognition (psychology)",
                "Object detection",
                "Convolution (computer science)",
                "Convolutional neural network",
                "Frame (networking)",
                "Pyramid (geometry)",
                "Feature (linguistics)",
                "Algorithm",
                "Artificial neural network",
                "Mathematics",
                "Telecommunications",
                "Linguistics",
                "Philosophy",
                "Geometry"
            ],
            "scholarly_citations_count": 3,
            "author_count": 4
        },
        {
            "lens_id": "065-305-068-192-13X",
            "title": "mSphere of Influence: the Rise of Artificial Intelligence in Infection Biology.",
            "year_published": 2019,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Bacterial colony",
                "Infection biology",
                "Computer science"
            ],
            "keywords": [
                "anthrax",
                "artificial intelligence",
                "bioimage analysis",
                "computer vision",
                "convolutional neural networks",
                "deep learning",
                "label-free imaging",
                "machine learning"
            ],
            "abstract": "ABSTRACT Artur Yakimovich works in the field of computational virology and applies machine learning algorithms to study host-pathogen interactions. In this mSphere of Influence article, he reflects on two papers \u201cHolographic Deep Learning for Rapid Optical Screening of Anthrax Spores\u201d by Jo et al. (Y. Jo, S. Park, J. Jung, J. Yoon, et al., Sci Adv 3:e1700606, 2017, https://doi.org/10.1126/sciadv.1700606) and \u201cBacterial Colony Counting with Convolutional Neural Networks in Digital Microbiology Imaging\u201d by Ferrari and colleagues (A. Ferrari, S. Lombardi, and A. Signoroni, Pattern Recognition 61:629\u2013640, 2017, https://doi.org/10.1016/j.patcog.2016.07.016). Here he discusses how these papers made an impact on him by showcasing that artificial intelligence algorithms can be equally applicable to both classical infection biology techniques and cutting-edge label-free imaging of pathogens.",
            "scholarly_citations_count": 7,
            "author_count": 1
        },
        {
            "lens_id": "066-445-511-382-589",
            "title": "Depth Selection for Deep ReLU Nets in Feature Extraction and Generalization",
            "year_published": 2022,
            "fields_of_study": [
                "Empirical risk minimization",
                "Deep learning",
                "Machine learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Generalization",
                "Feature engineering",
                "Feature (machine learning)",
                "Computer science",
                "Feature extraction",
                "Feature learning"
            ],
            "abstract": "Deep learning is recognized to be capable of discovering deep features for representation learning and pattern recognition without requiring elegant feature engineering techniques by taking advantage of human ingenuity and prior knowledge. Thus it has triggered enormous research activities in machine learning and pattern recognition. One of the most important challenge of deep learning is to figure out relations between a feature and the depth of deep neural networks (deep nets for short) to reflect the necessity of depth. Our purpose is to quantify this feature-depth correspondence in feature extraction and generalization. We present the adaptivity of features to depths and vice-verse via showing a depth-parameter trade-off in extracting both single feature and composite features. Based on these results, we prove that implementing the classical empirical risk minimization on deep nets can achieve the optimal generalization performance for numerous learning tasks. Our theoretical results are verified by a series of numerical experiments including toy simulations and a real application of earthquake seismic intensity prediction.",
            "scholarly_citations_count": 6,
            "author_count": 4
        },
        {
            "lens_id": "066-925-045-551-56X",
            "title": "Temperament detection based on Twitter data: classical machine learning versus deep learning",
            "year_published": 2022,
            "fields_of_study": [
                "Artificial intelligence",
                "Machine learning",
                "Computer science",
                "Deep learning",
                "Convolutional neural network",
                "Support vector machine",
                "Artificial neural network",
                "Context (archaeology)",
                "Dropout (neural networks)",
                "Paleontology",
                "Biology"
            ],
            "abstract": "<jats:p>Deep learning has shown promising results in various text-based classification tasks. However, deep learning performance is affected by the number of data, i.e., when the number of data is small, deep learning algorithms do not perform well, and vice versa. Classical machine learning algorithms commonly work well for a few data, and their performance reaches an optimal value and does not increase with the increase in sample data. Therefore, this study aimed to compare the performance of classical machine learning and deep learning methods to detect temperament based on Indonesian Twitter. In this study, the proposed Indonesian Linguistic Inquiry and Word Count were employed to analyze the context of Twitter. The classical machine learning methods implemented were support vector machine and K-nearest neighbor, whereas the deep learning method employed was a convolutional neural network (CNN) with three different architectures. Both learning methods were implemented using multiclass classification and one versus all (OVA) multiclass classification. The highest average f-measure was 58.73%, obtained by CNN OVA with a pool size of 3, a dropout value of 0.7, and a learning rate value of 0.0007.</jats:p>",
            "author_count": 4
        },
        {
            "lens_id": "075-448-027-402-031",
            "title": "Literature Survey on Application of Deep Learning in Face Recognition",
            "year_published": 2022,
            "fields_of_study": [
                "Computer science",
                "Artificial intelligence",
                "Deep learning",
                "Facial recognition system",
                "Convolutional neural network",
                "Face (sociological concept)",
                "Machine learning",
                "Artificial neural network",
                "Pattern recognition (psychology)",
                "Social science",
                "Sociology"
            ],
            "abstract": "Deep learning is an emerging technique in artificial intelligence in which a machine tries to copy the learning capability of human brain. As deep learning tries to mimic human brain it has the capability of overcoming many challenges in face recognition like change in expressions, pose, illumination, age etc. to an extent. This research explores some of the most advanced deep learning methods for face recognition. The algorithms that are being reviewed in this article are based on various techniques such as data augmentation, generation of synthetic training data, batch normalisation, linear binary pattern network, and others, which are used in conjunction with conventional Convolutional Neural Network to improve the performance of the algorithms. When comparing the approaches presented in this article to traditional face recognition algorithms, it can be concluded that the performance of face recognition is significantly enhanced.",
            "author_count": 3
        },
        {
            "lens_id": "078-151-718-511-503",
            "title": "Deep kernel learning in core vector machines",
            "year_published": 2017,
            "fields_of_study": [
                "Graph kernel",
                "Machine learning",
                "Kernel method",
                "Tree kernel",
                "Least squares support vector machine",
                "Relevance vector machine",
                "Artificial intelligence",
                "Polynomial kernel",
                "Computer science",
                "String kernel",
                "Radial basis function kernel"
            ],
            "abstract": "In machine learning literature, deep learning methods have been moving toward greater heights by giving due importance in both data representation and classification methods. The recently developed multilayered arc-cosine kernel leverages the possibilities of extending deep learning features into the kernel machines. Even though this kernel has been widely used in conjunction with support vector machines (SVM) on small-size datasets, it does not seem to be a feasible solution for the modern real-world applications that involve very large size datasets. There are lot of avenues where the scalability aspects of deep kernel machines in handling large dataset need to be evaluated. In machine learning literature, core vector machine (CVM) is being used as a scaling up mechanism for traditional SVMs. In CVM, the quadratic programming problem involved in SVM is reformulated as an equivalent minimum enclosing ball problem and then solved by using a subset of training sample (Core Set) obtained by a faster $$(1+\\epsilon )$$\n approximation algorithm. This paper explores the possibilities of using principles of core vector machines as a scaling up mechanism for deep support vector machine with arc-cosine kernel. Experiments on different datasets show that the proposed system gives high classification accuracy with reasonable training time compared to traditional core vector machines, deep support vector machines with arc-cosine kernel and deep convolutional neural network.",
            "scholarly_citations_count": 8,
            "author_count": 2
        },
        {
            "lens_id": "080-351-999-194-978",
            "title": "Towards automatic threat detection: A survey of advances of deep learning within X-ray security imaging",
            "year_published": 2022,
            "fields_of_study": [
                "Deep learning",
                "Benchmark (computing)",
                "Artificial intelligence",
                "Data science",
                "Field (computer science)",
                "Computer science",
                "Object (computer science)",
                "Anomaly detection",
                "Unsupervised learning",
                "Segmentation",
                "Taxonomy (general)"
            ],
            "abstract": "Abstract X-ray security screening is widely used to maintain aviation/transport security, and its significance poses a particular interest in automated screening systems. This paper aims to review computerised X-ray security imaging algorithms by taxonomising the field into conventional machine learning and contemporary deep learning applications. The first part briefly discusses the classical machine learning approaches utilised within X-ray security imaging, while the latter part thoroughly investigates the use of modern deep learning algorithms. The proposed taxonomy sub-categorises the use of deep learning approaches into supervised and unsupervised learning, with a particular focus on object classification, detection, segmentation and anomaly detection tasks. The paper further explores well-established X-ray datasets and provides a performance benchmark. Based on the current and future trends in deep learning, the paper finally presents a discussion and future directions for X-ray security imagery.",
            "scholarly_citations_count": 30,
            "author_count": 2
        },
        {
            "lens_id": "082-165-078-995-643",
            "title": "A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",
            "year_published": 2021,
            "fields_of_study": [
                "Medical research",
                "Deep learning",
                "Artificial intelligence",
                "Accountability",
                "Transparency (behavior)",
                "Interpretability",
                "Computer science",
                "Reliability (statistics)",
                "Artificial neural network",
                "Categorization"
            ],
            "abstract": "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide \u201cobviously\u201d interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.",
            "scholarly_citations_count": 482,
            "author_count": 2
        },
        {
            "lens_id": "086-710-502-122-259",
            "title": "Tuberculosis detection using convolutional neural network",
            "year_published": 2020,
            "fields_of_study": [
                "Digital image processing",
                "Deep learning",
                "Artificial intelligence",
                "Pattern recognition",
                "Image resolution",
                "Workload",
                "Biological network",
                "Pulmonary tuberculosis",
                "Computer science",
                "Medical diagnosis",
                "Convolutional neural network"
            ],
            "abstract": "Background\r\nPulmonary tuberculosis is an infectious disease has become one of the ten leading causes of death globally. Increasing the number and variety of radiological examinations increases the workload of radiologists. This causes the radiologist to experience fatigue, and trigger an inaccurate diagnosis, missed or delayed diagnosis. Machine learning is a computational model with an algorithm that is similar to the structure and function of the biological network of the human brain. It's part of artificial intelligence that uses computer science to perform digital image processing with pattern recognition techniques. The algorithm in machine learning can calculate, recognize the pattern in the image, and make predictive diagnoses.\r\nObjective\r\nGenerate deep learning model that can classify the chest x-rays image as tuberculosis and normal, also have the same performance with radiologists.\r\nMethods\r\nThe deep learning model using Convolutional Neural Network (CNN) with the input image size and filter size variation has developed, then compared to the expert performance.\r\nResults\r\nObtained the optimum deep learning model using an image of 200 x 200 and 5 x 5 filter size that has an accuracy, sensitivity, specificity, precision, and AUC were 0.97, 0.9667, 0.975, 0.9831, and 0.971 with CI of 0.932-1.\r\nConclusion\r\nThe deep learning model has 98% classification similarity with expert has obtained.",
            "author_count": 3
        },
        {
            "lens_id": "088-558-429-166-069",
            "title": "Hindsight-aware deep reinforcement learning algorithm for multi-agent systems",
            "year_published": 2022,
            "fields_of_study": [
                "Hindsight bias",
                "Reinforcement learning",
                "Computer science",
                "Artificial intelligence",
                "Multi-agent system",
                "Computational intelligence",
                "Quality (philosophy)",
                "Machine learning",
                "Psychology",
                "Philosophy",
                "Epistemology",
                "Cognitive psychology"
            ],
            "author_count": 3
        },
        {
            "lens_id": "090-410-228-253-036",
            "title": "Recognition Technology of Athlete\u2019s Limb Movement Combined Based on the Integrated Learning Algorithm",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Algorithm",
                "Support vector machine",
                "Feature selection",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Computer science",
                "Ensemble learning",
                "Swing",
                "Inertial measurement unit",
                "Decision tree"
            ],
            "abstract": "Human motion recognition based on inertial sensor is a new research direction in the field of pattern recognition. It carries out preprocessing, feature selection, and feature selection by placing inertial sensors on the surface of the human body. Finally, it mainly classifies and recognizes the extracted features of human action. There are many kinds of swing movements in table tennis. Accurately identifying these movement modes is of great significance for swing movement analysis. With the development of artificial intelligence technology, human movement recognition has made many breakthroughs in recent years, from machine learning to deep learning, from wearable sensors to visual sensors. However, there is not much work on movement recognition for table tennis, and the methods are still mainly integrated into the traditional field of machine learning. Therefore, this paper uses an acceleration sensor as a motion recording device for a table tennis disc and explores the three-axis acceleration data of four common swing motions. Traditional machine learning algorithms (decision tree, random forest tree, and support vector) are used to classify the swing motion, and a classification algorithm based on the idea of integration is designed. Experimental results show that the ensemble learning algorithm developed in this paper is better than the traditional machine learning algorithm, and the average recognition accuracy is 91%.",
            "author_count": 2
        },
        {
            "lens_id": "093-453-600-735-672",
            "title": "Handwritten isolated Bangla compound character recognition: A new benchmark using a novel deep learning approach",
            "year_published": 2017,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Support vector machine",
                "Benchmark (computing)",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Pattern recognition",
                "Word error rate",
                "Computer science",
                "Artificial neural network",
                "Convolutional neural network"
            ],
            "abstract": "Abstract In this work, a novel deep learning technique for the recognition of handwritten Bangla isolated compound character is presented and a new benchmarkof recognition accuracy on the CMATERdb 3.1.3.3 dataset is reported. Greedy layer wise training of Deep Neural Network has helped to make significant strides in various pattern recognition problems. We employ layerwise training to Deep Convolutional Neural Networks (DCNN) in a supervised fashion and augment the training process with the RMSProp algorithm to achieve faster convergence. We compare results with those obtained from standard shallow learning methods with predefined features, as well as standard DCNNs. Supervised layerwise trained DCNNs are found to outperform standard shallow learning models such as Support Vector Machines as well as regular DCNNs of similar architecture by achieving error rate of 9.67% thereby setting a new benchmark on the CMATERdb 3.1.3.3 with recognition accuracy of 90.33%, representing an improvement of nearly 10%.",
            "scholarly_citations_count": 98,
            "author_count": 4
        },
        {
            "lens_id": "095-016-929-841-697",
            "title": "Artificial intelligence for melanoma diagnosis: from support vector machines to deep learning and beyond",
            "year_published": 2021,
            "fields_of_study": [
                "Decision support system",
                "Deep learning",
                "Support vector machine",
                "Transfer of learning",
                "Artificial intelligence",
                "Overfitting",
                "Interpretability",
                "Computer science",
                "Artificial neural network",
                "Convolutional neural network"
            ],
            "abstract": "Melanoma diagnosis is a vexing problem, with practitioners requiring years of training and experience to achieve proficiency. Machine learning represents a disruptive force in this and other disciplines that rely on visual pattern recognition. Transfer learning using convolutional neural networks represents the state-of-the-art in artificial melanoma diagnosis, achieving similar sensitivity and specificity to experts. (1) Several machine learning algorithms, from support vector machines to neural networks, have also been applied to decision support which can guide biopsy decision-making by general practitioners and inexperienced clinicians. (2)\r\n\r\nThis presentation outlines the spectrum of artificial intelligence technologies which have shown promise for melanoma diagnosis and decision support, including recent results using deep learning to predict the need for lesion excision. This is followed by a review of current challenges facing the use of machine learning in clinical practice. Inherent difficulties include shortcomings in training datasets, risks of overfitting and poor inductive biases. There are also broader issues with model interpretability and an uncertain regulatory environment which must be resolved. (3)",
            "author_count": 2
        },
        {
            "lens_id": "096-286-610-042-97X",
            "title": "A review: Study of various techniques of Hand Gestures Recognition",
            "year_published": 2021,
            "abstract": "<jats:p>With the development of Artificial Intelligence based augmented reality systems, pattern and gestures recognitions have evolved exponentially over time. They provide the flexibility to incorporate human gestures into digital world information that gives us another mode of human computer interaction. This paper presents an overview on several techniques opted by modern researchers to realize hand gestures recognition process. This whole process involves four steps comprising preprocessing of input frame, segmentation, features extraction and recognition process. In recent years, deep learning algorithms (Convolutional neural networks and Artificial neural networks) are the core attention to researchers because of their robust nature and ease of use. On the other hand, ordinary machine learning algorithms are less accurate and requires a lot of preprocessing beforehand but they are computationally faster than deep learning algorithms. This paper deals with a comparative analysis of statistical gestures recognition techniques and machine learning techniques.</jats:p>"
        },
        {
            "lens_id": "097-951-647-293-741",
            "title": "Survey on Object Tracking using Deep Learning Paradigms",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Video tracking",
                "Computer vision",
                "Computer science"
            ],
            "abstract": "<jats:p>The field of object tracking extends across different domains. It is a major key player in the field of image processing and pattern recognition. Object tracking is the process of tracking an object over a continuous sequence of image frames to determine over time the relative movements or changes.\u00a0 With the massive advancements in the field of deep learning, the use of deep neural networks has risen due to their impressive accomplishments in object detection and tracking. In this Survey, the objective is to give a comprehensive overview of the recent attempts in the field of object tracking with a focus on the use of deep learning techniques and algorithms. The paper is divided into four sections; at first, we will give an overview of the recent work to highlight the techniques and methods which have been used in object tracking using deep learning. The second section focus is on the object tracking that uses convolutional networks techniques. The third section focuses on some of the recurrent neural networks to tack objects. The final section is concentrated on auto-encoders object tracking.</jats:p>",
            "scholarly_citations_count": 1,
            "author_count": 1
        },
        {
            "lens_id": "099-680-713-685-534",
            "title": "Comparison between multi-class classifiers and deep learning with focus on industry 4.0",
            "year_published": 2016,
            "fields_of_study": [
                "Deep learning",
                "Statistical classification",
                "Machine learning",
                "Artificial intelligence",
                "Instance-based learning",
                "Data science",
                "Industry 4.0",
                "Computer science",
                "Data modeling",
                "Active learning (machine learning)",
                "Cloud computing",
                "Data stream mining"
            ],
            "abstract": "Growing amounts of data will be one of consequences in Industry 4.0. This paper deals about mining frequent patterns and important factors in data. Classification is one of the most common assignments in data analytics. We used letter recognition data from the UCI repository as data set for our experiment. Data set contains more than 20000 instances of 26 classes. In our case, it represents multi-class classification. This idea can be transformed into industrial environment. Deep learning is a new area of machine learning research. We decided to use Deep learning from open source H2O machine learning framework and compare it with four multi-class classification algorithms available as services on Microsoft Azure. We are focusing this idea on Industrial systems, cloud architecture and data analytics, which will be fundamental pillars of Industry 4.0.",
            "scholarly_citations_count": 28,
            "author_count": 2
        },
        {
            "lens_id": "100-111-313-461-797",
            "title": "Deep Learning and Convolutional Neural Networks for Medical Image Computing - Review of Deep Learning Methods in Mammography, Cardiovascular, and Microscopy Image Analysis",
            "year_published": 2017,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Medical physics",
                "Data science",
                "Clinical Practice",
                "Mammography",
                "Deep neural networks",
                "Imaging modalities",
                "Disease detection",
                "Medical imaging",
                "Medicine",
                "Segmentation"
            ],
            "abstract": "Computerized algorithms and solutions in processing and diagnosis mammography X-ray, cardiovascular CT/MRI scans, and microscopy image play an important role in disease detection and computer-aided decision-making. Machine learning techniques have powered many aspects in medical investigations and clinical practice. Recently, deep learning is emerging a leading machine learning tool in computer vision and begins attracting considerable attentions in medical imaging. In this chapter, we provide a snapshot of this fast growing field specifically for mammography, cardiovascular, and microscopy image analysis . We briefly explain the popular deep neural networks and summarize current deep learning achievements in various tasks such as detection, segmentation, and classification in these heterogeneous imaging modalities. In addition, we discuss the challenges and the potential future trends for ongoing work.",
            "scholarly_citations_count": 33,
            "author_count": 4
        },
        {
            "lens_id": "101-794-919-172-297",
            "title": "Machine Learning for Efficient Assessment and Prediction of Human Performance in Collaborative Learning Environments",
            "year_published": 2018,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Collaborative learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Computer science",
                "Complex system",
                "Feature extraction",
                "Identification (information)",
                "Convolutional neural network",
                "Systems architecture"
            ],
            "abstract": "The objective of this work is to propose a machine learning-based methodology system architecture and algorithms to find patterns of learning, interaction, and relationship and effective assessment for a complex system involving massive data that could be obtained from a proposed collaborative learning environment (CLE). Collaborative learning may take place between dyads or larger team members to find solutions for real-time events or problems, and to discuss concepts or interactions during situational judgment tasks (SJT). Modeling a collaborative, networked system that involves multimodal data presents many challenges. This paper focuses on proposing a Machine Learning - (ML)-based system architecture to promote understanding of the behaviors, group dynamics, and interactions in the CLE. Our framework integrates techniques from computational psychometrics (CP) and deep learning models that include the utilization of convolutional neural networks (CNNs) for feature extraction, skill identification, and pattern recognition. Our framework also identifies the behavioral components at a micro level, and can help us model behaviors of a group involved in learning.",
            "scholarly_citations_count": 8,
            "author_count": 4
        },
        {
            "lens_id": "103-537-668-168-669",
            "title": "ICCCNT - A Threat of Deepfakes as a Weapon on Digital Platform and their Detection Methods",
            "year_published": 2021,
            "fields_of_study": [
                "Human\u2013computer interaction",
                "Deep learning",
                "Imitation",
                "Artificial intelligence",
                "Deception",
                "Counterfeit",
                "Facial recognition system",
                "Computer science",
                "Artificial neural network",
                "Voting"
            ],
            "abstract": "Advances in machine learning, deep learning, and Artificial Intelligence(AI) allows people to exchange other people's faces and voices in videos to make it look like what they did or say whatever you want to say. These videos and photos are called \u201cdeepfake\u201d and are getting more complicated every day and this has lawmakers worried. This technology uses machine learning technology to provide computers with real data about images, so that we can make forgeries. The creators of Deepfake use artificial intelligence and machine learning algorithms to mimic the work and characteristics of real humans. It differs from counterfeit traditional media because it is difficult to identify. As In the 2020 elections loomed, AI-generated deepfakes were hit the news cycle. DeepFakes threatens facial recognition and online content. This deception can be dangerous, because if used incorrectly, this technique can be abused. Fake video, voice, and audio clips can do enormous damage. This paper examines the algorithms used to generate deepfakes as well as the methods proposed to detect them. We go through the threats, research patterns, and future directions for deepfake technologies in detail. This research provides a detailed description of deep imitation technology and encourages the creation of new and more powerful methods to deal with increasingly severe deep imitation by studying the history of deep imitation.",
            "author_count": 2
        },
        {
            "lens_id": "104-014-543-404-844",
            "title": "Artificial Neural Networks Applied to Natural Language Processing in Academic Texts",
            "year_published": 2022,
            "fields_of_study": [
                "Artificial intelligence",
                "Computer science",
                "Artificial neural network",
                "Field (mathematics)",
                "Machine learning",
                "Task (project management)",
                "Information processing",
                "Robotics",
                "Data processing",
                "Deep learning",
                "Natural language processing",
                "Robot",
                "Database",
                "Engineering",
                "Mathematics",
                "Systems engineering",
                "Neuroscience",
                "Pure mathematics",
                "Biology"
            ],
            "abstract": "AbstractIn the last decade, Artificial Intelligence (AI) has become lead on the field of information generation and processing tasks through the emergence of Machine Learning (ML), as well as the data specialist mentions Machine Learning is a master of pattern recognition, and is capable of transform a data sample into a computer program capable of drawing inferences from new data sets for which it has not been previously trained, based on artificial neural networks (ANN) processing in academic texts, which are used to identify patterns and classify different types of information, currently treated as Deep Learning (DL) which is a subset of Machine Learning, this algorithm tries to imitate the human brain by continuously analyzing data with a given logical structure, which has led to its applicability to different fields such as robotics, voice processing, artificial vision, natural language processing (NLP), with the intention to provide computer systems with the ability to learn. Natural language processing has traditionally been a complex and non-trivial task in algorithm design. Making use of AI, new thresholds are being reached in the state of the art of different problems and with constant advances in the models in use, they are being reached faster and faster.KeywordsArtificial neural networksNatural language processingAcademic textsMachine learningDeep learning",
            "author_count": 4
        },
        {
            "lens_id": "107-871-049-392-329",
            "title": "Significance of Machine Learning and Deep Learning in Development of Artificial Intelligence",
            "year_published": 2022,
            "fields_of_study": [
                "Artificial intelligence",
                "Machine learning",
                "Deep learning",
                "Computer science",
                "Artificial neural network",
                "Field (mathematics)",
                "Hyper-heuristic",
                "Deep belief network",
                "Algorithmic learning theory",
                "Robot learning",
                "Unsupervised learning",
                "Robot",
                "Mathematics",
                "Pure mathematics",
                "Mobile robot"
            ],
            "abstract": "In the recent decades, the application of \u201cmachine learning\u201d, \u201cdeep learning\u201d and \u201cartificial intelligence\u201d has become widespread. In pattern recognition, deep artificial neural networks and machine learning have a wide range of applications. All the phrases are used often, sometimes interchangeably, with differing significance, in science and in media. Artificial intelligence is the technology that comprises science and engineering in making intelligent computer programs that make intelligent machines. Here, strong artificial intelligence makes use of machine learning and deep learning. Without programming explicitly, the ability of a system that improves by learning automatically and from its own experience is named as machine learning. Deep learning is an increasing field of research in machine learning (ML). And it is a subfield of machine learning that uses algorithms inspired by human brain with the help of large data sets using artificial neural networks. It consists of several occult layers of artificial neural networks. The approach of profound study employs high-level models and nonlinear transformations in huge databases. Recent improvements in artificial intelligence using deep learning architectures in several sectors have already contributed significantly to this. This study seeks to elucidate the link between the above words and to specify, in particular, the contribution to artificial intelligence by machine education and profound learning. We examine the relevant literature and provide a conceptual framework that explains the role of machine learning and profound learning in the development of intelligent (artificial) beings. In addition, the higher and more advantageous approach and the hierarchy of deep learning are given in layers and nonlinear operations in common applications and contrasted with the more standard techniques. We therefore want to give additional terminology and a basis for (interdisciplinary) talks.",
            "author_count": 6
        },
        {
            "lens_id": "108-811-539-421-454",
            "title": "Facial Expression Recognition Based on Deep Learning: A Survey",
            "year_published": 2017,
            "fields_of_study": [
                "Network model",
                "Deep learning",
                "Machine learning",
                "Artificial intelligence",
                "Pattern recognition",
                "Biometrics",
                "Facial expression",
                "Facial expression recognition",
                "Computer science",
                "Feature extraction",
                "Feature learning",
                "Affective computing"
            ],
            "abstract": "Facial expression recognition (FER) enables computers to understand human emotions and is the basis and prerequisite for quantitative analysis of human emotions. As a challenging interdisciplinary in biometrics and emotional computing, FER has become a research hotspot in the field of pattern recognition, computer vision and artificial intelligence both at home and abroad. As a new machine learning theory, deep learning not only emphasizes the depth of learning model, but also highlights the importance of feature learning for network model, and has made some research achievements in facial expression recognition. In this paper, the current research states are analyzed mostly from the latest facial expression extraction algorithm and the FER algorithm based on deep learning a comparison is made of these methods. Finally, the research challenges are generally concluded, and the possible trends are outlined.",
            "scholarly_citations_count": 33,
            "author_count": 1
        },
        {
            "lens_id": "109-568-481-618-631",
            "title": "Deep Learning for Political Science",
            "year_published": 2020,
            "abstract": "Political science, and social science in general, have traditionally been using computational methods to study areas such as voting behavior, policy making, international conflict, and international development. More recently, increasingly available quantities of data are being combined with improved algorithms and affordable computational resources to predict, learn, and discover new insights from data that is large in volume and variety. New developments in the areas of machine learning, deep learning, natural language processing (NLP), and, more generally, artificial intelligence (AI) are opening up new opportunities for testing theories and evaluating the impact of interventions and programs in a more dynamic and effective way. Applications using large volumes of structured and unstructured data are becoming common in government and industry, and increasingly also in social science research. This chapter offers an introduction to such methods drawing examples from political science. Focusing on the areas where the strengths of the methods coincide with challenges in these fields, the chapter first presents an introduction to AI and its core technology - machine learning, with its rapidly developing subfield of deep learning. The discussion of deep neural networks is illustrated with the NLP tasks that are relevant to political science. The latest advances in deep learning methods for NLP are also reviewed, together with their potential for improving information extraction and pattern recognition from political science texts",
            "scholarly_citations_count": 4,
            "author_count": 2
        },
        {
            "lens_id": "115-376-286-055-803",
            "title": "Review of Research on Biomedical Image Processing Based on Pattern Recognition",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Pattern recognition",
                "Basic research",
                "Image pattern recognition",
                "Biomedical image",
                "Common framework",
                "Computer science",
                "Biomedicine",
                "Image processing"
            ],
            "abstract": "Pattern recognition algorithms can discover valuable information from mass data of biomedical images as guide for basic research and clinical application. In recent years, with improvement of the theory and practice of pattern recognition and machine learning, especially the appearance and application of deep learning, the crossing researches among artificial intelligence, pattern recognition, and biomedicine become a hotspot, and achieve many breakthrough successes in related fields. This review introduces briefly the common framework and algorithms of image pattern recognition, summarizes the applications of these algorithms to biomedical image analysis including fluorescence microscopic images, histopathological images, and medical radiological images, and finally analyzes and prospect several potential research directions.",
            "author_count": 2
        },
        {
            "lens_id": "115-564-994-116-476",
            "title": "Comparative analysis of image classification algorithms based on traditional machine learning and deep learning",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Algorithm",
                "Machine learning",
                "Support vector machine",
                "Artificial intelligence",
                "MNIST database",
                "Field (computer science)",
                "Computer science",
                "Feedforward neural network",
                "Contextual image classification",
                "Convolutional neural network",
                "Image processing"
            ],
            "scholarly_citations_count": 137,
            "author_count": 3
        },
        {
            "lens_id": "117-703-963-052-799",
            "title": "Building and Interpreting Deep Similarity Models.",
            "year_published": 2022,
            "abstract": "Many learning algorithms such as kernel machines, nearest neighbors, clustering, or anomaly detection, are based on distances or similarities. Before similarities are used for training an actual machine learning model, we would like to verify that they are bound to meaningful patterns in the data. In this paper, we propose to make similarities interpretable by augmenting them with an explanation. We develop BiLRP, a scalable and theoretically founded method to systematically decompose the output of an already trained deep similarity model on pairs of input features. Our method can be expressed as a composition of LRP explanations, which were shown in previous works to scale to highly nonlinear models. Through an extensive set of experiments, we demonstrate that BiLRP robustly explains complex similarity models, e.g., built on VGG-16 deep neural network features. Additionally, we apply our method to an open problem in digital humanities: detailed assessment of similarity between historical documents, such as astronomical tables. Here again, BiLRP provides insight and brings verifiability into a highly engineered and problem-specific similarity model.",
            "scholarly_citations_count": 13,
            "author_count": 6
        },
        {
            "lens_id": "119-271-063-165-559",
            "title": "A Study of Deep Learning: Architecture, Algorithm and Comparison",
            "year_published": 2019,
            "fields_of_study": [
                "Signal processing",
                "Deep learning",
                "Algorithm",
                "Architecture",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Point (typography)",
                "Stochastic gradient descent",
                "Perception",
                "Computer science",
                "Artificial neural network"
            ],
            "abstract": "Machine learning highly comes with the broad concept of Deep Learning which is most widely using nowadays. Machine Learning is the study of inspiring PCs to learn and act like people do, and enhance their learning after some time in self-sufficient mold, by nourishing them information and data as perceptions and true connections. Deep Learning portrays a way to deal with discovering that is described by dynamic commitment, inherent inspiration, and an individual scan for significance\u201d. Deep Learning is the study of crossing point in the mid of exploration zones of neural networks, Artificial intelligence, graphical modeling, optimization, pattern recognition and Signal processing. The vital explanation behind the notoriety of Deep learning today are the radically expanded chip preparing ability (general-purpose graphical handling units GP-GPUs), altogether expanded size of the information utilized for preparing and continous evolving in the machine learning and flag/data preparing research. The study reveals the concept, different types of deep learning architecture, algorithms and applications of deep learning. This paper gives a simple view for the researchers in deep learning.",
            "author_count": 2
        },
        {
            "lens_id": "120-415-166-742-22X",
            "title": "Assessment of Optimizers impact on Image Recognition with Convolutional Neural Network to Adversarial Datasets",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Gradient descent",
                "Artificial intelligence",
                "Optimization algorithm",
                "Adversarial system",
                "Computer science",
                "Convolutional neural network"
            ],
            "abstract": "<jats:title>Abstract</jats:title>\n               <jats:p>In Artificial Intelligence, the machine modeling technique means to behave in the manner of human reflects indistinguishable. To automatizes the development of rational model for data evaluation, machine learning mechanism of artificial intelligence, is used. Deep learning is the machine learning discipline, having objective to imbide the system and to discover pattern from input. In pattern recognition, deep learning has paramount importance and different advance powerful model\u2019s architecture. The most effective, vital, and influential innovation in computer vision discipline is one of the architectures of deep learning called convolutional neural network. In this neural network various optimizers can be used for model molding into its appropriate form by weights futzing. Aiming to overcome the problem in getting the optimized result, the research used various algorithms of weight optimization. The article elaborates convolutional network concept as well as the idea behind the use of optimizers. Furthermore, the detailed study of optimizers is also presented in this paper. Along with it, experimental comparison and result of different learning paradigm optimizers is shown in the document. Considering the different image datasets, including MNIST, and CIFAR 10 dataset, the accuracies of convolutional model with different optimizers are verified.</jats:p>",
            "scholarly_citations_count": 2,
            "author_count": 4
        },
        {
            "lens_id": "124-539-489-743-315",
            "title": "Matter and memory and deep learning",
            "year_published": 2017,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Cognitive science",
                "Referent",
                "Meaning (existential)",
                "Dream",
                "Will contest",
                "Field (computer science)",
                "Computer science",
                "Phenomenon",
                "Turing"
            ],
            "abstract": "The recent phenomenon of \u2018Deep Learning,\u2019 which has given us such science-fiction-like innovations as search tools in photographic applications and the growing reality of self-driving cars, is a new form, and subset, of \u2018Machine Learning\u2019 made possible by very recent innovations in computing. Machine Learning itself has been around for some decades \u2013 essentially pattern-recognition software that requires very substantial computing resources, which were, until very recently, mostly theoretical and hard to come by. Machine Learning was one avenue of the field of Artificial Intelligence known as Narrow A.I. \u2013 the kind of \u2018artificial intelligence\u2019 that was strictly limited in scope as a first-steps starting point of what came, as a result, to be known as General A.I. General A.I., known then as simply, \u2018Artificial Intelligence\u2019, was the 1950s dream that brought us such things as Robbie the Robot, and more recently C3PO, and The Terminator: the kind of science fiction characters that remain the only manifestations of General Artificial Intelligence.\r\n\r\n\u2018Deep Learning\u2019 also continues engineering\u2019s 1940s trend of using language in a way that I will contest in this paper: a co-opting of words that have been used, in the past, to describe human activities, using them instead to describe what engineers have managed to make machines do. These co-optations reduce the richness of the word, making its referent an algorithm: a flow diagram that represents the bare essentials of what an engineer can understand and reproduce of a human activity; not the human activity itself. This diagram of the \u2018engineering possible\u2019 over-simplifies the human activity it tries to depict. With continued usage, the meaning of the word for us today has all-too-often become reduced to what the engineer has newly defined it to mean: something much less than it once was. \r\n\r\nIn this paper I propose to attempt to roll back some of these co-optations, and to re-introduce some of the richness of the words that have been taken by engineering. I shall examine Turing\u2019s seminal paper on the notion of a thinking machine. I shall be using the philosophical insights of Henri Bergson, especially in his book, Matter and Memory, and the discoveries of neuroscience and complexity scientists. I will try to show that the answer to Turing\u2019s question, \u2018Can machines think?\u2019 remains a resounding, \u2018No!\u2019, and that notions such as \u2018deep learning\u2019 are in fact not only an inaccurate use of the very human experience of learning, but degrade the latter in using such a term.",
            "scholarly_citations_count": 3,
            "author_count": 1
        },
        {
            "lens_id": "125-186-398-431-960",
            "title": "HyperFace: A Deep Multi-Task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition.",
            "year_published": 2017,
            "abstract": "We present an algorithm for simultaneous face detection, landmarks localization, pose estimation and gender recognition using deep convolutional neural networks (CNN). The proposed method called, HyperFace, fuses the intermediate layers of a deep CNN using a separate CNN followed by a multi-task learning algorithm that operates on the fused features. It exploits the synergy among the tasks which boosts up their individual performances. Additionally, we propose two variants of HyperFace: (1)\u00a0HyperFace-ResNet that builds on the ResNet-101 model and achieves significant improvement in performance, and (2)\u00a0Fast-HyperFace that uses a high recall fast face detector for generating region proposals to improve the speed of the algorithm. Extensive experiments show that the proposed models are able to capture both global and local information in faces and performs significantly better than many competitive algorithms for each of these four tasks.",
            "scholarly_citations_count": 561,
            "author_count": 3
        },
        {
            "lens_id": "129-267-873-530-758",
            "title": "Deep learning - cancer genetics and application of deep learning to cancer oncology",
            "year_published": 2022,
            "abstract": "<jats:p>Arguably the human body has been one of the most sophisticated systems we encounter but until now we are still far from understanding its complexity. We have been trying to replicate human intelligence by way of artificial intelligence but with limited success. We have discovered the molecular structure in terms of genetics, performed gene editing to change an organism\u2019s DNA and much more, but their translatability into the field of oncology has remained limited. Conventional machine learning methods achieved some degree of success in solving problems that we do not have an explicit algorithm. However, they are basically shallow learning methods, not rich enough to discover and extract intricate features that represent patterns in the real environment. Deep learning has exceeded human performance in pattern recognition as well as strategic games and are powerful for dealing with many complex problems. High-throughput sequencing and microarray techniques have generated vast amounts of data and allowed the comprehensive study of gene expression in tumor cells. The application of deep learning with molecular data enables applications in oncology with information not available from clinical diagnosis. This paper provides fundamental concepts of deep learning, an essential knowledge of cancer genetics, and a review of applications of deep learning to cancer oncology. Importantly, it provides an insightful knowledge of deep learning and an extensive discussion on its challenges. The ultimate purpose is to germinate ideas and facilitate collaborations between cancer biologists and deep learning researchers to address challenging oncological problems using advanced deep learning technologies.</jats:p>",
            "author_count": 2
        },
        {
            "lens_id": "131-588-812-323-092",
            "title": "Canadian Conference on AI - Deep Super Learner: A Deep Ensemble for Classification Problems",
            "year_published": 2018,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Structure (mathematical logic)",
                "Computer science",
                "Ensemble learning",
                "Artificial neural network",
                "Convergence (routing)",
                "Big data",
                "Transparency (human\u2013computer interaction)"
            ],
            "abstract": "Deep learning has become very popular for tasks such as predictive modeling and pattern recognition in handling big data. Deep learning is a powerful machine learning method that extracts lower level features and feeds them forward for the next layer to identify higher level features that improve performance. However, deep neural networks have drawbacks, which include many hyper-parameters and infinite architectures, opaqueness into results, and relatively slower convergence on smaller datasets. While traditional machine learning algorithms can address these drawbacks, they are not typically capable of the performance levels achieved by deep neural networks. To improve performance, ensemble methods are used to combine multiple base learners. Super learning is an ensemble that finds the optimal combination of diverse learning algorithms. This paper proposes deep super learning as an approach which achieves log loss and accuracy results competitive to deep neural networks while employing traditional machine learning algorithms in a hierarchical structure. The deep super learner is flexible, adaptable, and easy to train with good performance across different tasks using identical hyper-parameter values. Using traditional machine learning requires fewer hyper-parameters, allows transparency into results, and has relatively fast convergence on smaller datasets. Experimental results show that the deep super learner has superior performance compared to the individual base learners, single-layer ensembles, and in some cases deep neural networks. Performance of the deep super learner may further be improved with task-specific tuning.",
            "scholarly_citations_count": 29,
            "author_count": 3
        },
        {
            "lens_id": "132-400-619-360-472",
            "title": "The History of Machine Learning and Its Convergent Trajectory Towards\n            <scp>AI</scp>",
            "year_published": 2022,
            "fields_of_study": [
                "Artificial intelligence",
                "Computer science",
                "Machine learning",
                "Artificial neural network",
                "Boosting (machine learning)",
                "Process (computing)",
                "Instance-based learning",
                "Deep learning",
                "Stability (learning theory)",
                "Operating system"
            ],
            "abstract": "First came the goal of artificial intelligence, which eventually led to the development of machine learning (ML), as a means of achieving that goal. The emphasis on \u2018learning\u2019 in ML allows computers to make better decisions, based on previous experiences. It uses neural network models and algorithms to support the decision-making process. In the 1960s, researchers focused on developing ML algorithms to solve mathematical problems. In the late 1960s, machine \u2018vision\u2019 learning became a popular line of research. ML was used in computer vision, with the goal of object and facial recognition. The nearest neighbour algorithm came about in 1967, initiating the use of basic pattern recognition. In 1968\u20131969, the nearest neighbour algorithm evolved into the K nearest neighbour algorithm. Deep learning, as a branch of ML, employs algorithms to process data and imitate the thinking process, or to develop abstractions. \u2018Boosting\u2019, as an evolutionary step in the process of machine thinking, is significant.",
            "author_count": 1
        },
        {
            "lens_id": "135-077-610-261-015",
            "title": "Dynamic Deep Learning Algorithm (DDLA) for Processing of Complex and Large Datasets",
            "year_published": 2022,
            "fields_of_study": [
                "Computer science",
                "Artificial intelligence",
                "Deep learning",
                "Machine learning",
                "Convolutional neural network",
                "Classifier (UML)",
                "Autoencoder",
                "Artificial neural network",
                "Pattern recognition (psychology)"
            ],
            "abstract": "Nowadays, deep learning (DL) is blueprint for the machine learning (ML). Compare with NL, DL is most efficient, time and with low cost There are no limitations for the learning approaches in DL. DL algorithms can extract high-quality of features based on the dataset provided. Due to the fast development in internet technology, huge data is started processing by dividing according to the requirements. DL algorithms are more compatible for processing of huge and complex datasets such as pattern matching, recognition of handwriting, recognition of speeches, analysis of stock markets and many more. DL has more advancement on various applications and this will solve various issues in complicated pattern applications. Previously, datasets such as handwritten is used to find the accurate result with the Ensemble novel classifier (ENC). But this not worked up to the mark. In this paper, dynamic deep learning algorithm (DDLA) is designed and developed to process the complicated and complex datasets. This is the combination of Auto Encoder (AE) and Adaptive Convolutional neural network (A-CNN). Experimental results show the performance of the EDDLA with ENC.",
            "scholarly_citations_count": 1,
            "author_count": 2
        },
        {
            "lens_id": "136-928-653-752-585",
            "title": "Role of deep learning techniques in non-invasive diagnosis of human diseases.",
            "year_published": 2022,
            "fields_of_study": [
                "Artificial intelligence",
                "Machine learning",
                "Deep learning",
                "Computer science",
                "Convolutional neural network",
                "Feature engineering",
                "Modalities",
                "Raw data",
                "Multi-task learning",
                "Engineering",
                "Social science",
                "Systems engineering",
                "Sociology",
                "Programming language",
                "Task (project management)"
            ],
            "abstract": "Machine learning, a sub-discipline in the domain of artificial intelligence, concentrates on algorithms able to learn and/or adapt their structure (e.g., parameters) based on a set of observed data. The adaptation is performed by optimizing over a cost function. Machine learning obtained a great attention in the biomedical community because it offers a promise for improving sensitivity and/or specificity of detection and diagnosis of diseases. It also can increase objectivity of the decision making, decrease the time and effort on health care professionals during the process of disease detection and diagnosis. The potential impact of machine learning is greater than ever due to the increase in medical data being acquired, the presence of novel modalities being developed and the complexity of medical data. In all of these scenarios, machine learning can come up with new tools for interpreting the complex datasets that confront clinicians. Much of the excitement for the application of machine learning to biomedical research comes from the development of deep learning which is modeled after computation in the brain. Deep learning can help in attaining insights that would be impossible to obtain through manual analysis. Deep learning algorithms and in particular convolutional neural networks are different from traditional machine learning approaches. Deep learning algorithms are known by their ability to learn complex representations to enhance pattern recognition from raw data. On the other hand, traditional machine learning requires human engineering and domain expertise to design feature extractors and structure data. With increasing demands upon current radiologists, there are growing needs for automating the diagnosis. This is a concern that deep learning is able to address. In this dissertation, we present four different successful applications of deep learning for diseases diagnosis. All the work presented in the dissertation utilizes medical images. In the first application, we introduce a deep-learning based computer-aided diagnostic system for the early detection of acute renal transplant rejection. The system is based on the fusion of both imaging markers (apparent diffusion coefficients derived from diffusion-weighted magnetic resonance imaging) and clinical biomarkers (creatinine clearance and serum plasma creatinine). The fused data is then used as an input to train and test a convolutional neural network based classifier. The proposed system is tested on scans collected from 56 subjects from geographically diverse populations and different scanner types/image collection protocols. The overall accuracy of the proposed system is 92.9% with 93.3% sensitivity and 92.3% specificity in distinguishing",
            "author_count": 1
        },
        {
            "lens_id": "150-959-072-786-209",
            "title": "AAAI - DeepVar: An End-to-End Deep Learning Approach for Genomic Variant Recognition in Biomedical Literature",
            "year_published": 2020,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Natural language processing",
                "Feature engineering",
                "Named-entity recognition",
                "Computer science",
                "End-to-end principle"
            ],
            "abstract": "We consider the problem of Named Entity Recognition (NER) on biomedical scientific literature, and more specifically the genomic variants recognition in this work. Significant success has been achieved for NER on canonical tasks in recent years where large data sets are generally available. However, it remains a challenging problem on many domain-specific areas, especially the domains where only small gold annotations can be obtained. In addition, genomic variant entities exhibit diverse linguistic heterogeneity, differing much from those that have been characterized in existing canonical NER tasks. The state-of-the-art machine learning approaches heavily rely on arduous feature engineering to characterize those unique patterns. In this work, we present the first successful end-to-end deep learning approach to bridge the gap between generic NER algorithms and low-resource applications through genomic variants recognition. Our proposed model can result in promising performance without any hand-crafted features or post-processing rules. Our extensive experiments and results may shed light on other similar low-resource NER applications.",
            "author_count": 3
        },
        {
            "lens_id": "152-498-175-714-180",
            "title": "Performance Evaluation of Different Machine Learning Methods and Deep-Learning Based Convolutional Neural Network for Health Decision Making",
            "year_published": 2019,
            "fields_of_study": [
                "Deep learning",
                "Machine learning",
                "Image segmentation",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Health management system",
                "Factor (programming language)",
                "Computer science",
                "Component (UML)",
                "Artificial neural network",
                "Convolutional neural network"
            ],
            "abstract": "Now-a-days modern technology is used for health management and diagnostic strategy in the health sector. Machine learning usually helps in decision making for health issues using different models. Classification and prediction of disease are easily known with the help of machine learning techniques. The machine learning technique can be applied in various applications such as image segmentation, fraud detection, pattern recognition and disease prediction, etc. In the today\u2019s world, maximum people are suffering from diabetes. The glucose factor in the blood is the main component of diabetes. Fluctuation of blood glucose level leads to diabetes. To predict the diabetes disease, machine learning and deep learning play major role which uses probability, statistics and neural network concepts, etc. Deep learning is the part of machine learning which uses different layers of neural network that decide classification and prediction of disease. In this chapter, we study and compare among different machine learning algorithms and deep neural networks for diabetes disease prediction, by measuring performance. The experiment results prove that convolution neural network based deep learning method provides the highest accuracy than other machine learning algorithms.",
            "scholarly_citations_count": 44,
            "author_count": 3
        },
        {
            "lens_id": "154-576-609-006-469",
            "title": "KANDINSKYPatterns - An experimental exploration environment for Pattern Analysis and Machine Intelligence.",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Neocognitron",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Test data",
                "Closure (topology)",
                "Field (computer science)",
                "Computer science",
                "Concept learning",
                "Training set"
            ],
            "abstract": "Machine intelligence is very successful at standard recognition tasks when having high-quality training data. There is still a significant gap between machine-level pattern recognition and human-level concept learning. Humans can learn under uncertainty from only a few examples and generalize these concepts to solve new problems. The growing interest in explainable machine intelligence, requires experimental environments and diagnostic tests to analyze weaknesses in existing approaches to drive progress in the field. In this paper, we discuss existing diagnostic tests and test data sets such as CLEVR, CLEVERER, CLOSURE, CURI, Bongard-LOGO, V-PROM, and present our own experimental environment: The KANDINSKYPatterns, named after the Russian artist Wassily Kandinksy, who made theoretical contributions to compositivity, i.e. that all perceptions consist of geometrically elementary individual components. This was experimentally proven by Hubel &Wiesel in the 1960s and became the basis for machine learning approaches such as the Neocognitron and the even later Deep Learning. While KANDINSKYPatterns have computationally controllable properties on the one hand, bringing ground truth, they are also easily distinguishable by human observers, i.e., controlled patterns can be described by both humans and algorithms, making them another important contribution to international research in machine intelligence.",
            "scholarly_citations_count": 3,
            "author_count": 3
        },
        {
            "lens_id": "156-899-711-261-342",
            "title": "A Review of Deep Machine Learning",
            "year_published": 2016,
            "fields_of_study": [
                "Hyper-heuristic",
                "Machine learning",
                "Artificial intelligence",
                "Robot learning",
                "Learning classifier system",
                "Instance-based learning",
                "Algorithmic learning theory",
                "Computer science",
                "Computational learning theory",
                "Active learning (machine learning)",
                "Unsupervised learning"
            ],
            "abstract": "The rapid increase of information and accessibility in recent years has activated a paradigm shift in algorithm design for artificial intelligence. Recently, deep learning (a surrogate of Machine Learning) have won several contests in pattern recognition and machine learning. This review comprehensively summarises relevant studies, much of it from prior state-of-the-art techniques. This paper also discusses the motivations and principles regarding learning algorithms for deep architectures.",
            "scholarly_citations_count": 48,
            "author_count": 5
        },
        {
            "lens_id": "157-933-227-803-536",
            "title": "Machine Learning and Deep Learning for Photovoltaic Applications",
            "year_published": 2022,
            "fields_of_study": [
                "Photovoltaic system",
                "Computer science",
                "Artificial intelligence",
                "Engineering",
                "Electrical engineering"
            ],
            "abstract": "Artificial intelligence (AI) techniques including machine learning and deep learning algorithms have shown their capability in solving complex problems in different sectors such as, natural language processing, pattern recognition, forecasting, robotics, and other applications. Researchers working in the field of solar energy application both solar thermal and photovoltaic, are more interested to apply these techniques, and recently they are highly motivated especially by the DL algorithms usefulness. This chapter aims to show some applications of AI techniques, such as the k-nearest neighbours, neural networks, deep neural networks, fuzzy logic and long-short term memory networks, in photovoltaic (PV) systems. Various topics are covered such as current-voltage curves estimation, PV power forecasting, maximum power point tracking, and fault classification for PV systems. The examples presented are developed under the Matlab environment and Python programming language and show good prospects.",
            "scholarly_citations_count": 1,
            "author_count": 2
        },
        {
            "lens_id": "158-672-417-813-085",
            "title": "Network Intelligent Education System Based on the Deep Learning Algorithm",
            "year_published": 2022,
            "fields_of_study": [
                "Computer science",
                "Artificial intelligence",
                "Deep learning",
                "Machine learning",
                "Field (mathematics)",
                "Bayesian network",
                "Algorithm",
                "Support vector machine",
                "Data mining",
                "Mathematics",
                "Pure mathematics"
            ],
            "abstract": "<jats:p>In order to promote the deep integration of education and artificial intelligence and promote the landing of artificial intelligence in the field of education, a method based on deep learning is proposed. Deep learning is a very advanced pattern recognition algorithm, and it can mine potentially valuable knowledge from massive data and provide support for people\u2019s scientific decision-making. Experimental data show that the classification accuracy of deep learning algorithm reaches 99.5%. Even in the case of large data volume and large data categories, the accuracy is still up to 96.3%, which is far higher than the support vector machine algorithm, Bayesian theory algorithm, and K-means algorithm, which can mine and learn knowledge for users more accurately. Conclusion. The network intelligent education system based on deep learning strengthens the automation, intelligence, and interest of learning, so as to ensure the initiative of users in learning.</jats:p>",
            "author_count": 1
        },
        {
            "lens_id": "167-279-982-312-780",
            "title": "Optimization of Machine Learning and Deep Learning Algorithms for Diagnosis of Cancer",
            "year_published": 2022,
            "fields_of_study": [
                "Artificial intelligence",
                "Machine learning",
                "Deep learning",
                "Computer science",
                "Convolutional neural network",
                "Categorization",
                "Identification (biology)",
                "Artificial neural network",
                "Popularity",
                "Psychology",
                "Social psychology",
                "Botany",
                "Biology"
            ],
            "abstract": "<jats:p>Machine learning and artificial intelligence has recently become a prominent technology. Given its popularity and strength in pattern recognition and categorization, many corporations and institutions have begun investing in healthcare research to improve illness prediction accuracy. Using these strategies, however, has several drawbacks. One of the primary issues is the lack of huge datasets for medical pictures. An introduction to deep learning in medical image processing from theoretical foundations to real-world applications. The article examines the general appeal of deep learning (DL), a collection of computer science advances. The next step was to learn the basics of neural networks. That explains the use of deep learning and CNNs. So we can see why deep learning is rapidly advancing in various application fields, including medical image processing. The goal of this research was to use innovative methodologies on cancer datasets to explore the feasibility of combining machine learning and deep learning algorithms for cancer detection. This study used text and picture databases to classify cancer. This article provides optimization methods that outperform the suggested approaches' accuracy. Using two alternative training methods, Levenberg Marquardt (lm) and Resilient back propagation (rp), two classification algorithms were evaluated with different groups of neurons to identify benign and malignant patients. Cascade correlation utilizing the train (rp) outperformed feed forward back propagation using the train (lm). The second deep neural network model presented a technique (based on CNN) for automated brain tumor identification using MRI data.</jats:p>",
            "author_count": 4
        },
        {
            "lens_id": "170-741-182-079-12X",
            "title": "Sentiment analysis of Indonesian hotel reviews: from classical machine learning to deep learning",
            "year_published": 2021,
            "fields_of_study": [
                "Artificial intelligence",
                "Computer science",
                "Machine learning",
                "Word2vec",
                "Softmax function",
                "Deep learning",
                "Sentiment analysis",
                "Naive Bayes classifier",
                "Support vector machine",
                "Convolutional neural network",
                "Hyperparameter",
                "Embedding"
            ],
            "abstract": "<jats:p>Currently, there are a large number of hotel reviews on the Internet that need to be evaluated to turn the data into practicable information. Deep learning has excellent capabilities for recognizing this type of data. With the advances in deep learning paradigms, many algorithms have been developed that can be used in sentiment analysis tasks. In this study, we aim to compare the performance of classical machine learning algorithms\u2014logistic regression (LR), na\u00efve Bayes (NB), and support vector machine (SVM) using the Word2Vec model in conjunction with deep learning algorithms such as a convolutional neural network (CNN) to classify hotel reviews on the Traveloka website into positive or negative classes. Both learning methods apply hyperparameter tuning to determine the parameters that produce the best model. Furthermore, the Word2Vec model parameters use the skip-gram model, hierarchical softmax evaluation, and the value of 100 vector dimensions. The highest average accuracy obtained was 98.08% by using the CNN with a dropout of 0.2, Tanh as convolution activation, softmax as output activation, and Adam as the optimizer. The findings from the study demonstrate that the integration of the Word2Vec model and the CNN model obtains significantly better accuracy than other classical machine learning methods.</jats:p>",
            "scholarly_citations_count": 1,
            "author_count": 4
        },
        {
            "lens_id": "170-840-029-478-283",
            "title": "Deep learning evolution from shallow to deep: a case study based on language recognition",
            "year_published": 2022,
            "fields_of_study": [
                "Computer science",
                "Artificial intelligence",
                "Deep learning",
                "Artificial neural network",
                "Transformation (genetics)",
                "Deep neural networks",
                "Machine learning",
                "Natural language processing",
                "Biochemistry",
                "Chemistry",
                "Gene"
            ],
            "abstract": "Deep learning is a systems science based on neural network and machine learning. It abstracts complex data models by using multi-layer based on complex machine structures and nonlinear transformation algorithms. At present, the research results of deep learning have been successfully applied to speech recognition, pattern recognition, target recognition, natural language programming, man-machine chess, artificial intelligence, smart city and other fields. This paper mainly talks about how deep learning evolved and the evolution applied on speech recognition.",
            "author_count": 1
        },
        {
            "lens_id": "172-612-658-177-596",
            "title": "Deep Learning For Computer Vision Tasks: A review",
            "year_published": 2018,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Computer vision",
                "Field (computer science)",
                "Computer science",
                "Abstraction (linguistics)",
                "Object (computer science)",
                "Contextual image classification",
                "Identification (information)",
                "Noise (video)"
            ],
            "abstract": "Deep learning has recently become one of the most popular sub-fields of machine learning owing to its distributed data representation with multiple levels of abstraction. A diverse range of deep learning algorithms are being employed to solve conventional artificial intelligence problems. This paper gives an overview of some of the most widely used deep learning algorithms applied in the field of computer vision. It first inspects the various approaches of deep learning algorithms, followed by a description of their applications in image classification, object identification, image extraction and semantic segmentation in the presence of noise. The paper concludes with the discussion of the future scope and challenges for construction and training of deep neural networks.",
            "scholarly_citations_count": 13,
            "author_count": 3
        },
        {
            "lens_id": "173-822-946-930-820",
            "title": "Analysis of Brain Computer Interface Using Deep and Machine Learning",
            "year_published": 2022,
            "abstract": "<jats:p>Pattern recognition is becoming increasingly important topic in all sectors of society. From the optimization of processes in the industry to the detection and diagnosis of diseases in medicine. Brain-computer interfaces are introduced in this chapter. Systems capable of analyzing brain signal patterns, processing and interpreting them through machine and deep learning algorithms. In this chapter, a hybrid deep/machine learning ensemble system for brain pattern recognition is proposed. It is capable to recognize patterns and translate the decisions to BCI systems. For this, a public database (Physionet) with data of motor tasks and mental tasks is used. The development of this chapter consists of a brief summary of the state of the art, the presentation of the model together with some results and some promising conclusions.</jats:p>",
            "author_count": 2
        },
        {
            "lens_id": "174-250-225-520-122",
            "title": "Deep Learning",
            "year_published": 2020,
            "fields_of_study": [
                "Artificial intelligence",
                "Deep learning",
                "Computer science",
                "Artificial neural network",
                "Backpropagation",
                "Machine learning",
                "Deep belief network"
            ],
            "abstract": "This book focuses on the fundamentals of deep learning along with reporting on the current state-of-art research on deep learning. In addition, it provides an insight of deep neural networks in action with illustrative coding examples. Deep learning is a new area of machine learning research which has been introduced with the objective of moving ML closer to one of its original goals, i.e. artificial intelligence. Deep learning was developed as an ML approach to deal with complex input-output mappings. While traditional methods successfully solve problems where final value is a simple function of input data, deep learning techniques are able to capture composite relations between non-immediately related fields, for example between air pressure recordings and English words, millions of pixels and textual description, brand-related news and future stock prices and almost all real world problems. Deep learning is a class of nature inspired machine learning algorithms that uses a cascade of multiple layers of nonlinear processing units for feature extraction and transformation. Each successive layer uses the output from the previous layer as input. The learning may be supervised (e.g. classification) and/or unsupervised (e.g. pattern analysis) manners. These algorithms learn multiple levels of representations that correspond to different levels of abstraction by resorting to some form of gradient descent for training via backpropagation. Layers that have been used in deep learning include hidden layers of an artificial neural network and sets of propositional formulas. They may also include latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep boltzmann machines. Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision, automatic speech recognition (ASR) and human action recognition.",
            "scholarly_citations_count": 14
        },
        {
            "lens_id": "175-323-228-197-931",
            "title": "Drone Monitoring System to Detect Human Posture Using Deep Learning Algorithm",
            "year_published": 2021,
            "fields_of_study": [
                "Deep learning",
                "Algorithm",
                "Pattern recognition (psychology)",
                "Artificial intelligence",
                "Monitoring system",
                "Image detection",
                "Human life",
                "Computer science",
                "Drone",
                "Object (computer science)",
                "Convolutional neural network"
            ],
            "abstract": "Artificial Intelligence is taking part in many forms of daily human life. Many innovations have been embedded in technology to ensure they reach the same or even higher than human capability. One of the trending innovations in the market is image recognition. Image recognition is now implementing machine learning and deep learning for better image detection. Usual issues come for any image recognition technology about the low ability to detect various object classes. Too much research on image recognition also led to difficulty in applying the best algorithms. Although there are many technologies regarding image recognition, there is still not much work on human pose detection. Therefore, this paper proposed an application for detecting human pose related to drone control using a deep learning algorithm. This research aims to review the type of deep learning algorithm for human pose detection, develop an enhanced algorithm based on deep learning algorithm for human pose classification, and evaluate the proposed human pose classification algorithm's performance based on accuracy using drone technology. This paper proposes using Convolutional Neural Network (CNN) as a selected deep learning algorithm suitable for pattern recognition. This research expects an accurate result for detecting the human pose involving controlling the drone.",
            "author_count": 1
        },
        {
            "lens_id": "178-878-984-153-959",
            "title": "A survey on deep learning techniques in real-time applications",
            "year_published": 2022,
            "abstract": "<jats:p xml:lang=\"en\">In recent years, machine learning and Deep Learning have increased and gathered epic success in traditional application domains and new areas of Artificial Intelligence. The performance using Deep Learning has dominated experimental results compared to conventional machine learning algorithms. This paper presents an overview of the progress that has occurred in Deep Learning (DL) concerning some application domains like Autonomous Driving, Healthcare, Voice Recognition, Image Recognition, Advertising, Predicting Natural Calamities, National Stock Exchange and many more. Additionally, deeper insights into several Deep Learning techniques, their working principles, and experimental results are scrutinized. The survey covers Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL).</jats:p>",
            "author_count": 3
        },
        {
            "lens_id": "183-197-033-050-257",
            "title": "Incremental extreme learning machine based on deep feature embedded",
            "year_published": 2015,
            "fields_of_study": [
                "Deep learning",
                "Initialization",
                "Machine learning",
                "Artificial intelligence",
                "Population-based incremental learning",
                "Boltzmann machine",
                "Restricted Boltzmann machine",
                "Extreme learning machine",
                "Computer science",
                "Feature learning",
                "Deep belief network"
            ],
            "abstract": "Extreme learning machine (ELM) algorithm is used to train Single-hidden Layer Feed forward Neural Networks. And Deep Belief Network (DBN) is based on Restricted Boltzmann Machine (RBM). The conventional DBN algorithm has some insufficiencies, i.e., Contrastive Divergence (CD) Algorithm is not an ideal approximation method to Maximum Likelihood Estimation. And bad parameters selected in RBM algorithm will produce a bad initialization in DBN model so that we will spend more training time and get a low classification accuracy. To solve the problems above, we summarize the features of extreme learning machine and deep belief networks, and then propose Incremental extreme learning machine based on Deep Feature Embedded algorithm which combines the deep feature extracting ability of Deep Learning Networks with the feature mapping ability of extreme learning machine. Firstly, we introduce Manifold Regularization to our model to attenuate the complexity of probability distribution. Secondly, we introduce the semi-restricted Boltzmann machine (SRBM) to our algorithm, and build a deep belief network based on SRBM. Thirdly, we introduce the thought of incremental feature mapping in ELM to the classifier of DBN model. Finally, we show validity of the algorithm by experiments.",
            "scholarly_citations_count": 51,
            "author_count": 4
        },
        {
            "lens_id": "186-335-098-919-322",
            "title": "Deep Learning for Political Science",
            "year_published": 2020,
            "fields_of_study": [
                "Artificial intelligence",
                "Deep learning",
                "Computer science",
                "Variety (cybernetics)",
                "Data science",
                "Politics",
                "Voting",
                "Government (linguistics)",
                "Computational sociology",
                "Machine learning",
                "Political science",
                "Linguistics",
                "Philosophy",
                "Law"
            ],
            "abstract": "Political science, and social science in general, have traditionally been using computational methods to study areas such as voting behavior, policy making, international conflict, and international development. More recently, increasingly available quantities of data are being combined with improved algorithms and affordable computational resources to predict, learn, and discover new insights from data that is large in volume and variety. New developments in the areas of machine learning, deep learning, natural language processing (NLP), and, more generally, artificial intelligence (AI) are opening up new opportunities for testing theories and evaluating the impact of interventions and programs in a more dynamic and effective way. Applications using large volumes of structured and unstructured data are becoming common in government and industry, and increasingly also in social science research. This chapter offers an introduction to such methods drawing examples from political science. Focusing on the areas where the strengths of the methods coincide with challenges in these fields, the chapter first presents an introduction to AI and its core technology - machine learning, with its rapidly developing subfield of deep learning. The discussion of deep neural networks is illustrated with the NLP tasks that are relevant to political science. The latest advances in deep learning methods for NLP are also reviewed, together with their potential for improving information extraction and pattern recognition from political science texts.",
            "author_count": 2
        },
        {
            "lens_id": "188-863-964-492-583",
            "title": "A facial-expression recognition model using deep learning",
            "year_published": 2019,
            "fields_of_study": [
                "Deep learning",
                "Artificial intelligence",
                "Real image",
                "Pattern recognition",
                "Normalization (image processing)",
                "Deep neural networks",
                "Facial expression recognition",
                "Computer science"
            ],
            "abstract": "Deep neural networks have been recently putting a breakthrough in pattern recognition, machine learning and artificial intelligence. This paper emphasizes a study based on deep learning framework contributing to the field of expression recognition. The proposed model involves a technique using deep for human facial expression recognition. Images are first preprocessed with normalization manipulation to remove illumination and facilitate enhancement using hat-filtering. At that point, a weighted, focus symmetric nearby paired example (CS-LBP) is connected to each face hinder by piece. The CS-LBP pieces are connected to form an element vector of the face picture. The deep network is trained using the layer-wise strategy. We use the CIFAR-10 dataset is used for training and testing. A database of real images is used for testing the algorithms. GUI has been created which compares trained and tested dataset and specifies the type of expression in the command window.",
            "author_count": 2
        },
        {
            "lens_id": "191-836-358-380-658",
            "title": "Sooty Tern Optimization Algorithm-Based Deep Learning Model for Diagnosing NSCLC Tumours.",
            "year_published": 2023,
            "fields_of_study": [
                "Lung cancer",
                "Segmentation",
                "Artificial intelligence",
                "Computer science",
                "Algorithm",
                "Reliability (semiconductor)",
                "Domain (mathematical analysis)",
                "Feature (linguistics)",
                "Cancer",
                "Medicine",
                "Pathology",
                "Mathematics",
                "Power (physics)",
                "Internal medicine",
                "Mathematical analysis",
                "Linguistics",
                "Physics",
                "Philosophy",
                "Quantum mechanics"
            ],
            "keywords": [
                "deep learning model",
                "lung cancer",
                "non-small cell lung cancer (NSCLC)",
                "sooty tern optimization algorithm (STOA)"
            ],
            "abstract": "Lung cancer is one of the most common causes of cancer deaths in the modern world. Screening of lung nodules is essential for early recognition to facilitate treatment that improves the rate of patient rehabilitation. An increase in accuracy during lung cancer detection is vital for sustaining the rate of patient persistence, even though several research works have been conducted in this research domain. Moreover, the classical system fails to segment cancer cells of different sizes accurately and with excellent reliability. This paper proposes a sooty tern optimization algorithm-based deep learning (DL) model for diagnosing non-small cell lung cancer (NSCLC) tumours with increased accuracy. We discuss various algorithms for diagnosing models that adopt the Otsu segmentation method to perfectly isolate the lung nodules. Then, the sooty tern optimization algorithm (SHOA) is adopted for partitioning the cancer nodules by defining the best characteristics, which aids in improving diagnostic accuracy. It further utilizes a local binary pattern (LBP) for determining appropriate feature retrieval from the lung nodules. In addition, it adopts CNN and GRU-based classifiers for identifying whether the lung nodules are malignant or non-malignant depending on the features retrieved during the diagnosing process. The experimental results of this SHOA-optimized DNN model achieved an accuracy of 98.32%, better than the baseline schemes used for comparison.",
            "author_count": 6
        },
        {
            "lens_id": "198-116-366-213-189",
            "title": "Analysis of Machine Learning and Deep Learning Algorithms for Detection of Brain Disorders Using MRI Data",
            "year_published": 2022,
            "fields_of_study": [
                "Artificial intelligence",
                "Naive Bayes classifier",
                "Machine learning",
                "Random forest",
                "Support vector machine",
                "Decision tree",
                "Computer science",
                "Schizophrenia (object-oriented programming)",
                "Data set",
                "Neuroimaging",
                "Magnetic resonance imaging",
                "Algorithm",
                "Medicine",
                "Psychiatry",
                "Radiology",
                "Programming language"
            ],
            "abstract": "AbstractBrain diseases impact more than 1 billion people worldwide and include a wide spectrum of diseases and disorders such as stroke, Alzheimer\u2019s, Parkinson\u2019s, Epilepsy and other Seizure disorders. Most of these brain illnesses are subjected to misclassification, and early diagnosis increases the possibilities of preventing or delaying the development of these disorders. Magnetic Resonance Imaging (MRI) plays an important role in the diagnosis of patients with brain disorders and offers the potential of non-invasive longitudinal monitoring and bio-markers of disease progression. Our work focuses on using machine learning and deep learning techniques for the preemptive diagnosis of Schizophrenia using Kaggle data set and Alzheimer\u2019s using TADPOLE data set comprising of MRI features. Since the number of works using TADPOLE data set is minimum, we have chosen this for our study. Machine learning algorithms such as support vector machine (SVM), Decision Tree, Random Forest, Gaussian Naive Bayes, and 1D-CNN deep learning algorithm have been used for the classification of the disorders. It has been observed that Gaussian NB performed the best on Schizophrenia data, while Random Forest outperformed on Alzheimer\u2019s data compared to the other classifiers.KeywordsMagnetic Resonance Imaging (MRI)Alzheimer\u2019sParkinson\u2019sBrain disorders1D-CNNGaussian Naive Bayes",
            "author_count": 8
        }
    ],
    "results": 100
}